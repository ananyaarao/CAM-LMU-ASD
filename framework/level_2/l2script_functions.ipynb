{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "566a0e82-0c28-42c5-ad5d-7e1aaff96ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from l2s2 (l2s2_TET_Data_Processing.ipynb)\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\"\"\"\n",
    "turning the x and y values obtaining for each day into a function as well\n",
    "\"\"\"\n",
    "def drop_lead_trail_nans(arr):\n",
    "    #boolean mask for non-NaN values\n",
    "    non_nan_mask = ~np.isnan(arr)    \n",
    "    #finding the indices of the first and last non-NaN values\n",
    "    first_non_nan = np.argmax(non_nan_mask)  #First True value (non-NaN)\n",
    "    last_non_nan = len(arr) - np.argmax(non_nan_mask[::-1])  #Last True value (non-NaN)    \n",
    "    # Slice the array to drop leading and trailing NaNs\n",
    "    cleaned_arr = arr[first_non_nan:last_non_nan]\n",
    "\n",
    "    return cleaned_arr\n",
    "\n",
    "def giv_x_y_vals(mainfolder, qnum, ger):\n",
    "    #ger = True if subject recruited in Germany. Else False\n",
    "    if ger:\n",
    "        question_dict = {\n",
    "        'q1' : 'Wie wach fühlten Sie sich im Tagesverlauf?',\n",
    "        'q2' : 'Wie gelangweilt fühlten Sie sich im Tagesverlauf?',\n",
    "        'q3' : 'Wie sehr haben Sie gezielt versucht  einen oder verschiedene Sinneseindrücke zu vermeiden (z.B. Geruch  Geschmack  Geräusche)?',\n",
    "        'q4' : 'Wie sehr haben Sie versucht  soziale Interaktion zu vermeiden (virtuell und/oder persönlich)?',\n",
    "        'q5' : 'Wie sehr haben Sie sich über den Tag hinweg körperlich angespannt gefühlt?',\n",
    "        'q6' : 'Wie sehr haben Sie sich Sorgen gemacht über gegenwärtige oder zukünftige Ereignisse/Erfahrungen?',\n",
    "        'q7' : 'Wie sehr haben Sie sich Sorgen oder Gedanken über vergangene Erfahrungen / Ereignisse gemacht?',\n",
    "        'q8' : 'Wie sehr haben Sie sich im Tagesverlauf gestresst gefühlt?',\n",
    "        'q9' : 'Wie sehr haben Sie körperliche Schmerzen im Tagesverlauf gehabt?',\n",
    "        'q10' : 'Falls Sie eine zusätzliche individuelle Erfahrung und ihre Dynamik ergänzen möchten  haben Sie hier Platz. Bitte ergänzen Sie den Namen der Erfahrung als Überschrift  sowie die Ausprägung der Intensität im Graphen links. '\n",
    "        }    \n",
    "    \n",
    "    \n",
    "    else:\n",
    "        question_dict = {\n",
    "        'q1' : 'Question 1: How alert did you feel during the day?',\n",
    "        'q2' : 'Question 2: How bored did you feel during the day?',\n",
    "        'q3' : 'Question 3: Were you avoiding stimulation of your senses (touch',\n",
    "        'q4' : 'Question 4: Were you avoiding social interactions (virtual and/or in person)?',\n",
    "        'q5' : 'Question 5: How physically tense did you feel throughout the day? ',\n",
    "        'q6' : 'Question 6: How much were you lost in thoughts worrying about present and future events (e.g.',\n",
    "        'q7' : 'Question 7: How much were you lost in thoughts worrying about past events (e.g.',\n",
    "        'q8' : 'Question 8: How stressed did you feel during the day?',\n",
    "        'q9' : 'Question 9: How strong did you feel physical pain?'\n",
    "        }\n",
    "    \n",
    "    dict_TET_x = {}\n",
    "    dict_TET_y = {}\n",
    "    if ger:\n",
    "        expr = 'TET – Tag';\n",
    "        # to avoid duplicates in question indices - that could happen if the subject erased graph data instead of submitting the graph\n",
    "        expr2 = 'Submit rating'; \n",
    "        # to make sure at least one data point has been entered by the subject\n",
    "        expr3 = 'Saved drawing data'\n",
    "    else:\n",
    "        expr = 'Daily Experience';\n",
    "        # to avoid duplicates in question indices - that could happen if the subject erased graph data instead of submitting the graph\n",
    "        expr2 = 'Submit rating'; \n",
    "        # to make sure at least one data point has been entered by the subject\n",
    "        expr3 = 'Saved drawing data'\n",
    "\n",
    "\n",
    "    for folder in sorted(os.listdir(mainfolder)):\n",
    "        print(folder)\n",
    "        if folder.endswith('.png') or folder.endswith('.csv') or folder.endswith('.txt') or folder.endswith('.xlsx') or folder.endswith('.pdf')or folder.endswith('.edf'):\n",
    "            continue\n",
    "        \"\"\"    \n",
    "        if folder != '18_3_24_n10_19_3_24_d': #'15_3_24_n7_16_3_24_d':\n",
    "            continue\n",
    "        \"\"\"    \n",
    "        for file in os.listdir(os.path.join(mainfolder, folder)):\n",
    "            if file.endswith('_corrected.xlsx'):\n",
    "                print(file)\n",
    "                df_tet = pd.read_excel(os.path.join(mainfolder, folder, file))\n",
    "                if 'MENÜ' in df_tet.columns:\n",
    "                    pl1 = 'MENÜ'\n",
    "                    pl2 = 'Press button: Tägliche Erfahrungen'\n",
    "                    pl3 = 'Unnamed: 3'\n",
    "                else:\n",
    "                    pl1 = 'Category'\n",
    "                    pl2 = 'Action'\n",
    "                    pl3 = 'Question'\n",
    "            \n",
    "                #where expr and expr2 are found in the same row i.e, the rows containing the questions\n",
    "                mask_tet = (df_tet[pl1] == expr) & (df_tet[pl2] == expr2)\n",
    "                indices_tet = df_tet.index[mask_tet].tolist()\n",
    "                ind = 0\n",
    "                for i in range(0,len(indices_tet)):\n",
    "                    #print(i)\n",
    "                    if i == len(indices_tet)-1:\n",
    "                        i1 = len(df_tet)\n",
    "                        print('yes', i)\n",
    "                    #if indices_tet[i]+1>=len(df_tet): #commented out due to unpredictable behaviour, re-introduce after correction\n",
    "                     #   print(\"Warning! Below this last index, no x and y data may have been submitted! -> \", i)\n",
    "                      #  break\n",
    "                    else:\n",
    "                        i1 = indices_tet[i+1]\n",
    "                    if df_tet[pl3].iloc[indices_tet[i]] == question_dict[qnum]:\n",
    "                        if ind !=0:\n",
    "                            print(\"ind has already been assigned, assigning this new index to another variable x_new and y_new: \", indices_tet[i], \" this is its position in the indices list: \", i)\n",
    "                            ind = indices_tet[i]\n",
    "                            if qnum == 'q7':\n",
    "                                if df_tet[pl2].iloc[ind+1] != expr3 and df_tet[pl2].iloc[ind+2] != expr3:\n",
    "                                    print(\"Warning! Below this index, no x_new and y_new data may have been submitted therefore no data! -> \", ind)\n",
    "                                else:\n",
    "                                    x_new = df_tet['x_val'].iloc[indices_tet[i]:i1]\n",
    "                                    y_new = df_tet['y_val'].iloc[indices_tet[i]:i1]\n",
    "                                    print(\"x_new and y_new have been assigned and the correct index for q is ind which is: \", ind)\n",
    "\n",
    "                                    if np.array_equal(np.array(x_new), np.array(x)) and np.array_equal(np.array(y_new), np.array(y)):\n",
    "                                        print(\"both the old data and new data entered by the participant for the same question are identical, assigning new data to the dictionaries\")\n",
    "                                    else:\n",
    "                                        print(\"the old data and new data entered by the participant for the same question may not be identical! Checking the data more closely\")\n",
    "                                       \n",
    "                                        x_arr = drop_lead_trail_nans(np.array(x))\n",
    "                                        x_new_arr = drop_lead_trail_nans(np.array(x_new))\n",
    "                                        y_arr = drop_lead_trail_nans(np.array(y))\n",
    "                                        y_new_arr = drop_lead_trail_nans(np.array(y_new))\n",
    "        \n",
    "                                        if len(x_arr) != len(x_new_arr) or len(y_arr) != len(y_new_arr):\n",
    "                                            print(\"data not identical even after dropping leading and trailing nan values, assigning the new values to the dictionaries\")\n",
    "                                        else:\n",
    "                                            if np.sum(x_new_arr-x_arr) == 0 and np.sum(y_new_arr-y_arr) == 0:\n",
    "                                                print(\"data identical, but assigning the new values to dictionaries anyway\")\n",
    "                                            else: \n",
    "                                                print(\"data not identical after subtraction check, assigning the new values to the dictionaries\")\n",
    "                \n",
    "                                    dict_TET_x[folder] = np.array(x_new)\n",
    "                                    dict_TET_y[folder] = np.array(y_new)\n",
    "                            else:\n",
    "                                if df_tet[pl2].iloc[ind+1] != expr3:\n",
    "                                    print(\"Warning! Below this index, no x and y data may have been submitted therefore no data! -> \", ind)\n",
    "                                else:\n",
    "                                    x_new = df_tet['x_val'].iloc[indices_tet[i]:i1]\n",
    "                                    y_new = df_tet['y_val'].iloc[indices_tet[i]:i1]\n",
    "                                    print(\"x_new and y_new have been assigned and the correct index for q is ind which is: \", ind)\n",
    "                                    \n",
    "                                    if np.array_equal(np.array(x_new), np.array(x)) and np.array_equal(np.array(y_new), np.array(y)):\n",
    "                                        print(\"both the old data and new data entered by the participant for the same question are identical, assigning new data to the dictionaries\")   \n",
    "                                    else:\n",
    "                                        print(\"the old data and new data entered by the participant for the same question may not be identical! Checking the data more closely\")\n",
    "                                       \n",
    "                                        x_arr = drop_lead_trail_nans(np.array(x))\n",
    "                                        x_new_arr = drop_lead_trail_nans(np.array(x_new))\n",
    "                                        y_arr = drop_lead_trail_nans(np.array(y))\n",
    "                                        y_new_arr = drop_lead_trail_nans(np.array(y_new))\n",
    "        \n",
    "                                        if len(x_arr) != len(x_new_arr) or len(y_arr) != len(y_new_arr):\n",
    "                                            print(\"data not identical even after dropping leading and trailing nan values, assigning the new values to the dictionaries\")\n",
    "                                        else:\n",
    "                                            if np.sum(x_new_arr-x_arr) == 0 and np.sum(y_new_arr-y_arr) == 0:\n",
    "                                                print(\"data identical, but assigning the new values to dictionaries anyway\")\n",
    "                                            else: \n",
    "                                                print(\"data not identical after subtraction check, assigning the new values to the dictionaries\")\n",
    "                \n",
    "                                    dict_TET_x[folder] = np.array(x_new)\n",
    "                                    dict_TET_y[folder] = np.array(y_new)\n",
    "                        else:\n",
    "                            ind = indices_tet[i]\n",
    "                            x_new = 0\n",
    "                            y_new = 0\n",
    "                            if qnum == 'q7':\n",
    "                                if df_tet[pl2].iloc[ind+1] != expr3 and df_tet[pl2].iloc[ind+2] != expr3:\n",
    "                                    print(\"Warning! Below this index, no x and y data may have been submitted therefore no data! -> \", ind)\n",
    "                                else:\n",
    "                                    x = df_tet['x_val'].iloc[indices_tet[i]:i1]\n",
    "                                    y = df_tet['y_val'].iloc[indices_tet[i]:i1]\n",
    "                                    print(\"x and y have been assigned and the correct index for q is ind which is: \", ind)\n",
    "                \n",
    "                                    dict_TET_x[folder] = np.array(x)\n",
    "                                    dict_TET_y[folder] = np.array(y)\n",
    "                            else:\n",
    "                                if df_tet[pl2].iloc[ind+1] != expr3:\n",
    "                                    print(\"Warning! Below this index, no x and y data may have been submitted therefore no data! -> \", ind)\n",
    "                                else:\n",
    "                                    x = df_tet['x_val'].iloc[indices_tet[i]:i1]\n",
    "                                    y = df_tet['y_val'].iloc[indices_tet[i]:i1]\n",
    "                                    print(\"x and y have been assigned and the correct index for q is ind which is: \", ind)\n",
    "                \n",
    "                                    dict_TET_x[folder] = np.array(x)\n",
    "                                    dict_TET_y[folder] = np.array(y)\n",
    "    return dict_TET_x, dict_TET_y, x_new, y_new, x, y\n",
    "    \n",
    "def give_binned_vals(x_val, y_val, time_bin_val):\n",
    "    #time_bin_val is a string variable that should say either '15' or '30' or '60' to indicate 15 minute time bins or half hour time bins or one hour time bins\n",
    "    bin_dict = {}\n",
    "    if time_bin_val == '60':\n",
    "        bin_arr = np.arange(0,25)\n",
    "    elif time_bin_val == '30':\n",
    "        bin_arr = np.arange(0,24.5,0.5)\n",
    "    elif time_bin_val == '15': #did elif instead of else because this allows adding more time bin lengths in future if needed instead of else defaulting to 15 minute time bins. This will force the user to explicitly state the value of time bin\n",
    "        bin_arr = np.arange(0,24.25,0.25)\n",
    "    \n",
    "    for i in range(0, len(bin_arr) - 1):\n",
    "        #Create the key for the dictionary\n",
    "        key = str(bin_arr[i]) + '_' + str(bin_arr[i+1])\n",
    "    \n",
    "        #Initialize an empty list for this key\n",
    "        templst = []\n",
    "        bin_dict[key] = templst\n",
    "    \n",
    "        #Iterate over x_val, append to templst if condition is met\n",
    "        for j in range(0, len(x_val)):\n",
    "            if x_val[j] >= bin_arr[i] and x_val[j] < bin_arr[i+1]:\n",
    "                #Append y_val[j] directly to the list in the dictionary\n",
    "                bin_dict[key].append(y_val[j])\n",
    "\n",
    "    #for conversion of lists to numpy arrays\n",
    "    for key in bin_dict:\n",
    "        bin_dict[key] = np.array(bin_dict[key])\n",
    "    #print(bin_dict)\n",
    "\n",
    "    bin_dict_mean = {}\n",
    "    for key in bin_dict:\n",
    "        if len(bin_dict[key])!=0:\n",
    "            bin_dict_mean[key] = np.nanmean(bin_dict[key]) #modified to nanmean from mean even though there isn't a chance of getting nan values (just safety)\n",
    "        else:\n",
    "            bin_dict_mean[key] = -5000\n",
    "\n",
    "    return bin_dict_mean # bin_dict,\n",
    "\n",
    "\n",
    "\n",
    "def give_binned_vals_category(x_val, y_val):\n",
    "    bin_dict = {}\n",
    "    bin_arr = np.arange(0,25,6)\n",
    "    for i in range(0, len(bin_arr) - 1):\n",
    "        #Create the key for the dictionary\n",
    "        key = str(bin_arr[i]) + '_' + str(bin_arr[i+1])\n",
    "    \n",
    "        #Initialize an empty list for this key\n",
    "        templst = []\n",
    "        bin_dict[key] = templst\n",
    "    \n",
    "        #Iterate over x_val, append to templst if condition is met\n",
    "        for j in range(0, len(x_val)):\n",
    "            if x_val[j] >= bin_arr[i] and x_val[j] < bin_arr[i+1]:\n",
    "                #Append y_val[j] directly to the list in the dictionary\n",
    "                bin_dict[key].append(y_val[j])\n",
    "\n",
    "    #for conversion of lists to numpy arrays\n",
    "    for key in bin_dict:\n",
    "        bin_dict[key] = np.array(bin_dict[key])\n",
    "    #print(bin_dict)\n",
    "\n",
    "    bin_dict_mean = {}\n",
    "    for key in bin_dict:\n",
    "        if len(bin_dict[key])!=0:\n",
    "            bin_dict_mean[key] = np.nanmean(bin_dict[key]) #modified to nanmean from mean even though there isn't a chance of getting nan values (just safety)\n",
    "        else:\n",
    "            bin_dict_mean[key] = -5000\n",
    "\n",
    "    return bin_dict_mean # bin_dict,\n",
    "\n",
    "\"\"\"\n",
    "code block to record the start and end timestamps (in utc/unix as well as cet) in a nested dictionary format. The key would be the filename (with or without .csv), and the values would be the first and last timestamps as entered in the excel sheet. All the files generated for that day are to be included in order to obtain a quick overview.\n",
    "\"\"\"\n",
    "def get_start_end_timestamp(ipfileDir):\n",
    "    for file in os.listdir(ipfileDir):\n",
    "        if file.endswith('eda_cet.csv'):\n",
    "            df_eda = pd.read_csv(os.path.join(ipfileDir, file))\n",
    "            dict_time = {}\n",
    "            dict_time['start'] = [df_eda['unix_timestamp'].iloc[0], df_eda['CET_timestamp'].iloc[0]]\n",
    "            dict_time['end'] = [df_eda['unix_timestamp'].iloc[-1], df_eda['CET_timestamp'].iloc[-1]]\n",
    "            filenm = file\n",
    "    return dict_time, filenm\n",
    "\n",
    "\n",
    "def get_req_ips(parentfolder, folder2, dict_TET_map, question_dict):\n",
    "    key = parentfolder.split('\\\\')[-1]\n",
    "    \n",
    "    #extract x\n",
    "    ip_dict_TET_x = input('enter the main dictionary from which the required x values should be extracted: ')\n",
    "    if ip_dict_TET_x in dict_TET_map:\n",
    "        dict_TET_x = dict_TET_map[ip_dict_TET_x]\n",
    "        x = dict_TET_x[key]\n",
    "        print(\"x assigned\")\n",
    "    else:\n",
    "        print(\"invalid input, enter correct dictionary name\")\n",
    "        \n",
    "    #extract y\n",
    "    ip_dict_TET_y = input('enter the main dictionary from which the required y values should be extracted: ')\n",
    "    if ip_dict_TET_y in dict_TET_map:\n",
    "        dict_TET_y = dict_TET_map[ip_dict_TET_y]\n",
    "        y = dict_TET_y[key]\n",
    "        print(\"y assigned\")\n",
    "    else:\n",
    "        print(\"invalid input, enter correct dictionary name\")\n",
    "        \n",
    "    #get question number for the figure name when saving to folder\n",
    "    q = input('enter the question number (in the form q1, q2, q3, etc): ')\n",
    "    #from q, obtaining the question and hence the title for the figure\n",
    "    title = question_dict[eval(q)]\n",
    "    #get folder name in which to save \n",
    "    fig_folder = os.path.join(parentfolder, folder2)\n",
    "    \n",
    "    \n",
    "    return x, y, q, title, fig_folder   \n",
    "       \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8481729-3454-4398-bf51-8416d38e266a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from l2s3 (l2s3_EDA_Data_Preprocessing.ipynb)\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "import csv\n",
    "\n",
    "#pip install flirt\n",
    "\"\"\"\n",
    "flirt imports: optional\n",
    "\n",
    "import flirt.with_\n",
    "flirt.with_.me()\n",
    "import flirt.reader.empatica\n",
    "\n",
    "\n",
    "#now need function to one by one collect each raw avro file for a particular day, calculate eda fearures, get the mean of the columns tonic mean and phasic mean. These two measures for each day are to be collected in a dictionary. \n",
    "\n",
    "\n",
    "def give_means(file):\n",
    "    for file in os.listdir(ipfileDir):\n",
    "        if file.endswith('eda_cet.csv'):\n",
    "            df_eda_eplus = pd.read_csv(os.path.join(ipfileDir, file)) #filepath joining functionality \n",
    "            new_cet_index = pd.to_datetime(df_eda_eplus['CET_timestamp'])\n",
    "            df_eda_eplus.index = new_cet_index #indexing by date-time object to avoid error with flirt package\n",
    "            eda_feat_eplus = flirt.get_eda_features(df_eda_eplus['eda'],\n",
    "                                          window_length = 60, \n",
    "                                          window_step_size = 1,\n",
    "                                          data_frequency = 4)\n",
    "    return(eda_feat_eplus['tonic_mean'].mean(skipna=True), eda_feat_eplus['phasic_mean'].mean(skipna=True)) \n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.signal as scisig\n",
    "import pywt\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "\n",
    "matplotlib.rcParams['ps.useafm'] = True\n",
    "matplotlib.rcParams['pdf.use14corefonts'] = True\n",
    "matplotlib.rcParams['text.usetex'] = True\n",
    "\n",
    "def predict_binary_classifier(X):\n",
    "    ''''\n",
    "    X: num test data by 13 features\n",
    "    '''\n",
    "\n",
    "    # Get params\n",
    "    params = binary_classifier()\n",
    "\n",
    "    # compute kernel for all data points\n",
    "    K = rbf_kernel(params['support_vec'], X, gamma=params['gamma'])\n",
    "\n",
    "    # Prediction = sign((sum_{i=1}^n y_i*alpha*K(x_i,x)) + rho)\n",
    "    predictions = np.zeros(X.shape[0])\n",
    "    for i in range(X.shape[0]):\n",
    "        # prediction is calculated and then divided into positive (no artefact) or negative (artefact)\n",
    "        predictions[i] = np.sign(np.sum(params['dual_coef']*K[:, i]) + params['intercept'])\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def binary_classifier():\n",
    "    gamma = 0.1\n",
    "\n",
    "    # dual coef = y_i*alpha_i\n",
    "    dual_coef = np.array([[-1.12775599e+02,  -1.00000000e+03,  -1.00000000e+03,\n",
    "                           -1.00000000e+03,  -1.00000000e+03,  -1.00000000e+03,\n",
    "                           -1.00000000e+03,  -1.00000000e+03,  -1.00000000e+03,\n",
    "                           -1.00000000e+03,  -1.00000000e+03,  -1.00000000e+03,\n",
    "                           -4.65947457e+02,  -1.00000000e+03,  -1.00000000e+03,\n",
    "                           -1.00000000e+03,  -1.17935400e+02,  -1.00000000e+03,\n",
    "                           -1.00000000e+03,  -1.00000000e+03,  -1.00000000e+03,\n",
    "                           -1.00000000e+03,  -1.00000000e+03,  -1.00000000e+03,\n",
    "                           -1.00000000e+03,  -2.92534132e+02,  -1.00000000e+03,\n",
    "                           -1.00000000e+03,  -3.69965631e+01,  -1.00000000e+03,\n",
    "                           -1.00000000e+03,  -1.00000000e+03,  -1.00000000e+03,\n",
    "                           -1.00000000e+03,  -1.00000000e+03,  -1.00000000e+03,\n",
    "                           -1.00000000e+03,  -1.00000000e+03,   1.00000000e+03,\n",
    "                           1.00000000e+03,   1.00000000e+03,   1.00000000e+03,\n",
    "                           7.92366387e+02,   3.00553142e+02,   2.22950860e-01,\n",
    "                           1.00000000e+03,   1.00000000e+03,   5.58636056e+02,\n",
    "                           1.21751544e+02,   1.00000000e+03,   1.00000000e+03,\n",
    "                           2.61920652e+00,   9.96570403e+02,   1.00000000e+03,\n",
    "                           1.00000000e+03,   1.00000000e+03,   1.00000000e+03,\n",
    "                           1.00000000e+03,   1.00000000e+03,   1.02270060e+02,\n",
    "                           5.41288840e+01,   1.91650287e+02,   1.00000000e+03,\n",
    "                           1.00000000e+03,   1.00000000e+03,   1.00000000e+03,\n",
    "                           1.00000000e+03,   2.45152637e+02,   7.53766346e+02,\n",
    "                           1.00000000e+03,   1.00000000e+03,   3.63211198e+00,\n",
    "                           1.00000000e+03,   3.31675798e+01,   5.64620367e+02,\n",
    "                           1.00000000e+03,   1.00000000e+03,   1.00000000e+03,\n",
    "                           2.66900636e+02,   1.00000000e+03,   6.54763900e+02,\n",
    "                           3.38216549e+02,   6.86434772e+01,   2.78998678e+02,\n",
    "                           6.97557950e+02,   1.00000000e+03]])\n",
    "\n",
    "    # intercept = rho\n",
    "    intercept = np.array([-2.63232929])\n",
    "\n",
    "    # support vectors = x_i\n",
    "    support_vec = np.array([[0.02809756, 0.0455, 0.025, 0.00866667, 0.03799132, -0.00799413, 0.01061208, 0.016263, 0.00671743, 0.00572262, 0.00578504, 0.00542415, 0.00318195],\n",
    "                            [0.00060976, 0.0035, 0.007, 0.00087179, 0.00024191, -0.0005069, 0.0005069, 0.0070711, 0.00306413, 0.0031833, 0.0107827, 0.0066959, 0.0022981],\n",
    "                            [3.49731707, 0.092, 0.054, 0.01923077, 3.53815367, -0.02236652, 0.02659884, 0.062225, 0.0316782, 0.01818914, 0.06607571, 0.03342241, 0.099702],\n",
    "                            [2.52643902, 0.058, 0.055, 0.0114359, 2.54031008, -0.01070662, 0.01296803, 0.043134, 0.01649923, 0.01579683, 0.03326171, 0.05004163, 0.013965],\n",
    "                            [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, -2.74622599e-18, -2.42947453e-17, 3.36047450e-17, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
    "                            [3.89758537, 0.167, 0.27, 0.06717949, 3.87923565, -0.04130143, 0.05403825, 0.047376, 0.0328098, 0.01255584, 0.03676955, 0.14237773, 0.11031],\n",
    "                            [0.93326829, 0.0855, 0.106, 0.01169231, 0.92669874, -0.02740927, 0.02740927, 0.043841, 0.01131377, 0.01595008, 0.0231871, 0.02414775, 0.0139655],\n",
    "                            [4.64253659, 0.106, 0.13, 0.03661538, 4.63806066, -0.03168223, 0.03168223, 0.10182, 0.0559785, 0.03369301, 0.06341563, 0.08583294, 0.0251025],\n",
    "                            [0.29312195, 0.028, 0.039, 0.00682051, 0.28575076, -0.00648365, 0.00648365, 0.0056569, 0.00367694, 0.00126494, 0.00364005, 0.01814984, 0.006364],\n",
    "                            [3.08187805, 0.0615, 0.123, 0.03435897, 3.11862292, -0.02260403, 0.02260403, 0.053033, 0.0397394, 0.01570345, 0.0338851, 0.10069204, 0.16652],\n",
    "                            [2.43902439e-05, 5.00000000e-04, 1.00000000e-03, 1.02564103e-04, 2.43769719e-05, -7.19856842e-05, 7.19856842e-05, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
    "                            [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, -4.05052739e-10, -2.77557303e-09, 5.77955577e-09, 7.07110000e-04, 1.17851667e-04, 2.88676449e-04, 2.04124145e-04, 1.44336183e-04, 0.00000000e+00],\n",
    "                            [0.83290244, 0.099, 0.172, 0.02610256, 0.82408369, -0.0168393, 0.0168393, 0.13011, 0.02875613, 0.04987211, 0.03786379, 0.02684837, 0.0155565],\n",
    "                            [0.92597561, 0.017, 0.009, 0.00369231, 0.92583814, -0.00670974, 0.00670974, 0.012021, 0.00506763, 0.00420523, 0.01259266, 0.0115391, 0.00265165],\n",
    "                            [2.43902439e-05, 5.00000000e-04, 1.00000000e-03, 2.56410256e-05, 2.18000765e-04, -5.56411248e-04, 5.56411248e-04, 9.19240000e-03, 2.71058333e-03, 4.25246049e-03, 2.49833278e-03, 7.64311464e-03, 0.00000000e+00],\n",
    "                            [0.88760976, 0.0205, 0.022, 0.00489744, 0.88799505, -0.00346772, 0.00461828, 0.011314, 0.00447838, 0.00394135, 0.01327278, 0.01434142, 0.00406585],\n",
    "                            [9.21263415, 0.118, 0.472, 0.0695641, 9.19153391, -0.02181738, 0.02181738, 0.16688, 0.07130037, 0.06135461, 0.04328934, 0.04277416, 0.0829085],\n",
    "                            [0.48378049, 0.017, 0.026, 0.00794872, 0.48333175, -0.00337375, 0.00350864, 0.016971, 0.0089568, 0.00472601, 0.01168189, 0.01629524, 0.0226275],\n",
    "                            [0.00000000e+000, 0.00000000e+000, 0.00000000e+000, 0.00000000e+000, 9.65026603e-122, -2.00921455e-120, 4.22507597e-120, 0.00000000e+000, 0.00000000e+000, 0.00000000e+000, 0.00000000e+000, 0.00000000e+000, 0.00000000e+000],\n",
    "                            [0.10897561, 0.03, 0.033, 0.00553846, 0.12761266, -0.00442938, 0.00556735, 0.025456, 0.00872107, 0.00870258, 0.01130487, 0.01554551, 0.0123745],\n",
    "                            [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, -1.38812548e-09, -2.34438020e-08, 2.34438020e-08, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
    "                            [0.66663415, 0.052, 0.05, 0.00510256, 0.66182973, -0.01361869, 0.01361869, 0.0049497, 0.00296982, 0.00208565, 0.00424264, 0.00961131, 0.012374],\n",
    "                            [3.74146341e+00, 6.60000000e-02, 7.00000000e-02, 2.41025641e-02, 3.72790310e+00, -1.65194036e-02, 1.65194036e-02, 2.33350000e-02, 2.29102000e-02, 3.87787571e-04, 7.25086202e-03, 8.04828002e-03, 2.26270000e-02],\n",
    "                            [2.43902439e-05, 5.00000000e-04, 1.00000000e-03, 1.02564103e-04, 2.44149661e-05, -7.19856850e-05, 7.19856850e-05, 7.07110000e-04, 1.17851667e-04, 2.88676449e-04, 2.04124145e-04, 1.44336183e-04, 0.00000000e+00],\n",
    "                            [1.14713659e+01, 1.68000000e-01, 3.24000000e-01, 8.83589744e-02, 1.13977278e+01, -4.35202063e-02, 4.35202063e-02, 1.20920000e-01, 1.15826000e-01, 5.32593935e-03, 4.29825546e-02, 1.11681949e-01, 1.82080000e-01],\n",
    "                            [1.63631707, 0.0825, 0.138, 0.02410256, 1.65473267, -0.02914746, 0.02927458, 0.074953, 0.02899134, 0.03271076, 0.02718317, 0.09610564, 0.012728],\n",
    "                            [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 6.01460518e-42, -2.71490067e-40, 2.71490067e-40, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
    "                            [0.52358537, 0.038, 0.03, 0.00769231, 0.52319376, -0.01066405, 0.01066405, 0.026163, 0.01025307, 0.00912966, 0.02678697, 0.04011893, 0.00866185],\n",
    "                            [0.10931707, 0.103, 0.407, 0.04461538, 0.13188551, -0.01686662, 0.02506229, 0.1492, 0.0384195, 0.06327203, 0.06411448, 0.05508901, 0],\n",
    "                            [0.0444878, 0.0245, 0.04, 0.00984615, 0.03577326, -0.00573919, 0.00573919, 0.013435, 0.0078961, 0.00418135, 0.01136515, 0.01291603, 0.0134352],\n",
    "                            [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.03127202e-08, -2.56175141e-07, 5.37317466e-07, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
    "                            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "                            [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.27917545e-05, -7.79437718e-04, 7.79437718e-04, 3.04060000e-02, 5.06766667e-03, 1.24131975e-02, 1.34721936e-02, 5.34029589e-02, 0.00000000e+00],\n",
    "                            [2.43902439e-05, 5.00000000e-04, 1.00000000e-03, 1.02564103e-04, 2.60691650e-05, -7.19856850e-05, 7.19856850e-05, 7.07110000e-04, 1.17851667e-04, 2.88676449e-04, 2.04124145e-04, 1.44336183e-04, 0.00000000e+00],\n",
    "                            [0.46446341, 0.033, 0.03, 0.00933333, 0.46299034, -0.00866364, 0.00866364, 0.033941, 0.01357644, 0.01214903, 0.02164486, 0.02701617, 0.012374],\n",
    "                            [5.89978049, 0.117, 0.112, 0.04453846, 5.88525247, -0.02253416, 0.02253416, 0.084146, 0.0492146, 0.01985341, 0.06802812, 0.09041259, 0.045255],\n",
    "                            [0.01317073, 0.0195, 0.015, 0.00538462, 0.00829287, -0.00622806, 0.00622806, 0.026163, 0.01145514, 0.00926554, 0.00690652, 0.02540613, 0.018031],\n",
    "                            [1.16509756, 0.028, 0.02, 0.01051282, 1.16338281, -0.01379371, 0.01379371, 0.020506, 0.01461345, 0.00563317, 0.01416569, 0.01971055, 0.0281075],\n",
    "                            [3.67914634, 0.1235, 0.126, 0.02676923, 3.67052968, -0.04266586, 0.04266586, 0.041719, 0.0233342, 0.0106888, 0.03232337, 0.07260248, 0.050912],\n",
    "                            [0.11331707, 0.0015, 0.004, 0.0014359, 0.11329803, -0.00042144, 0.00042144, 0.0021213, 0.0014142, 0.00109543, 0.00124164, 0.00053231, 0.00070713],\n",
    "                            [1.11256098, 0.026, 0.016, 0.00561538, 1.09093248, -0.00174647, 0.00490015, 0.02192, 0.01272782, 0.00816993, 0.02111102, 0.04921207, 0.012021],\n",
    "                            [0.06846341, 0.007, 0.01, 0.00307692, 0.06774886, -0.00179795, 0.00190969, 0.0056569, 0.00311126, 0.00162791, 0.00195576, 0.00721732, 0.01096],\n",
    "                            [1.16454634e+01, 1.78500000e-01, 3.20000000e-01, 8.94615385e-02, 1.15869935e+01, -1.15451745e-02, 1.59897956e-02, 1.37890000e-01, 1.23393333e-01, 1.01170444e-02, 3.66151153e-02, 1.46607419e-01, 1.94455000e-01],\n",
    "                            [3.45158537, 0.1375, 0.052, 0.01676923, 3.44594643, -0.03141983, 0.03141983, 0.038184, 0.0272946, 0.00958649, 0.01698014, 0.06290749, 0.1393],\n",
    "                            [3.12563415, 0.0535, 0.111, 0.02897436, 3.17337638, -0.02835417, 0.02835417, 0.054447, 0.0278601, 0.0188188, 0.00755315, 0.03628251, 0.055154],\n",
    "                            [8.50975610e-02, 1.00000000e-03, 4.00000000e-03, 8.20512821e-04, 8.50491997e-02, -1.84870042e-04, 2.35933619e-04, 1.41420000e-03, 1.41420000e-03, 2.60312573e-11, 4.08248290e-04, 2.88668284e-04, 7.07110000e-04],\n",
    "                            [0.82373171, 0.048, 0.121, 0.01853846, 0.82149219, -0.0053288, 0.00684639, 0.041012, 0.0208598, 0.01423898, 0.02609294, 0.02676908, 0.01078335],\n",
    "                            [4.39680488, 0.223, 0.354, 0.09258974, 4.35973108, -0.03206468, 0.03450864, 0.20506, 0.0971572, 0.07235446, 0.13713059, 0.23019854, 0.32138],\n",
    "                            [5.66058537, 0.0285, 0.093, 0.01282051, 5.66682734, -0.00633008, 0.00633008, 0.040305, 0.01513214, 0.01889847, 0.01503912, 0.03383458, 0],\n",
    "                            [0.13329268, 0.011, 0.021, 0.00338462, 0.13419267, -0.00262455, 0.00262455, 0.0035355, 0.00226272, 0.00092195, 0.00772172, 0.00411547, 0.0038891],\n",
    "                            [0.15463415, 0.0325, 0.065, 0.01617949, 0.15422134, -0.00766504, 0.00766504, 0.067882, 0.02286322, 0.02270081, 0.02939288, 0.0224428, 0.017501],\n",
    "                            [1.47902439e-01, 1.50000000e-03, 2.00000000e-03, 3.84615385e-04, 1.48269290e-01, -1.36058722e-04, 1.36058722e-04, 2.12130000e-03, 8.24950000e-04, 9.39849132e-04, 5.16397779e-04, 5.91603500e-04, 0.00000000e+00],\n",
    "                            [2.76797561, 0.071, 0.17, 0.03212821, 2.84223399, -0.01692731, 0.01692731, 0.04879, 0.03441267, 0.00934515, 0.03221283, 0.05768286, 0.092806],\n",
    "                            [1.30939024, 0.044, 0.066, 0.0165641, 1.2967273, -0.01727205, 0.01727205, 0.03182, 0.01456652, 0.01056655, 0.00732632, 0.02987207, 0.038891],\n",
    "                            [0.0914878, 0.038, 0.028, 0.00364103, 0.08295897, -0.00877545, 0.00877545, 0.032527, 0.00648182, 0.01277828, 0.01289089, 0.01040763, 0.0042426],\n",
    "                            [0.13621951, 0.0015, 0.006, 0.00174359, 0.13689296, -0.00036169, 0.00040731, 0.0021213, 0.00153205, 0.00082663, 0.00058452, 0.00069522, 0.00088391],\n",
    "                            [0.05692683, 0.007, 0.006, 0.00189744, 0.05532006, -0.00145672, 0.00145672, 0.0056569, 0.00311126, 0.00184393, 0.00420714, 0.00465287, 0.0070711],\n",
    "                            [0.07460976, 0.002, 0.006, 0.00097436, 0.07430141, -0.00035004, 0.00038011, 0.0028284, 0.00113136, 0.0011832, 0.00070711, 0.0005916, 0.00070711],\n",
    "                            [0.04782927, 0.006, 0.011, 0.00353846, 0.04406202, -0.00232859, 0.00232859, 0.012021, 0.00438408, 0.00442728, 0.00363318, 0.00540593, 0.0091924],\n",
    "                            [4.443, 0.141, 0.076, 0.02310256, 4.40858239, -0.03710778, 0.03710778, 0.03182, 0.0271528, 0.00465324, 0.03506173, 0.07970664, 0.11278],\n",
    "                            [8.79678049, 0.057, 0.208, 0.04194872, 8.784878, -0.01132933, 0.01132933, 0.08061, 0.04695182, 0.039817, 0.0405623, 0.01937402, 0.033234],\n",
    "                            [2.58236585, 0.063, 0.128, 0.02112821, 2.5705713, -0.0079298, 0.01979542, 0.062225, 0.0309712, 0.02172778, 0.02949491, 0.02741888, 0.02687],\n",
    "                            [0.08992683, 0.0015, 0.006, 0.00030769, 0.09000535, -0.00020308, 0.00020308, 0.0021213, 0.00106065, 0.00116188, 0.0007746, 0.00086603, 0.00053035],\n",
    "                            [0.09085366, 0.0175, 0.037, 0.00694872, 0.09607742, -0.00456388, 0.00456388, 0.0098995, 0.00523258, 0.00310646, 0.01357571, 0.0133944, 0.0056569],\n",
    "                            [1.34473171, 0.0255, 0.022, 0.00953846, 1.37010789, -0.00558419, 0.00558419, 0.030406, 0.0134351, 0.00877511, 0.00929516, 0.03188089, 0.0265165],\n",
    "                            [0.14253659, 0.001, 0.004, 0.00097436, 0.14237889, -0.0002998, 0.0002998, 0.0014142, 0.0011785, 0.00057734, 0.0005164, 0.00069521, 0.00106066],\n",
    "                            [0.07617073, 0.001, 0.004, 0.00179487, 0.07597272, -0.00025949, 0.00025949, 0.0014142, 0.0011785, 0.00057734, 0.0005164, 0.00063245, 0.00070711],\n",
    "                            [0.28502439, 0.0025, 0.01, 0.00241026, 0.28596915, -0.000355, 0.000355, 0.12869, 0.02333393, 0.05162999, 0.0313152, 0.13233722, 0.0044194],\n",
    "                            [5.97658537, 0.0645, 0.106, 0.02925641, 5.95365623, -0.01454886, 0.01454886, 0.045962, 0.02913296, 0.02145587, 0.04602717, 0.06410626, 0.053033],\n",
    "                            [4.19787805, 0.0405, 0.072, 0.02764103, 4.21230508, -0.01456906, 0.01468492, 0.030406, 0.02206174, 0.01003006, 0.02031748, 0.03873656, 0.034295],\n",
    "                            [0.06904878, 0.0025, 0.005, 0.00117949, 0.06819891, -0.00023428, 0.00033805, 0.0035355, 0.00098994, 0.00154918, 0.001, 0.0007071, 0.00070711],\n",
    "                            [2.07410488e+01, 1.10000000e-02, 4.40000000e-02, 1.24102564e-02, 2.07288498e+01, -5.11402880e-02, 5.11402880e-02, 1.55560000e-02, 1.55560000e-02, 0.00000000e+00, 5.68037557e-03, 3.17543685e-03, 7.77820000e-03],\n",
    "                            [0.15141463, 0.0025, 0.008, 0.00161538, 0.15286961, -0.00066236, 0.00066236, 0.0049497, 0.0021213, 0.00180276, 0.00235584, 0.01268589, 0.0021213],\n",
    "                            [1.07970732, 0.0275, 0.046, 0.00725641, 1.0819483, -0.0025949, 0.00261392, 0.026163, 0.00754248, 0.00945165, 0.01400506, 0.00566908, 0.011137],\n",
    "                            [1.45278049e+00, 2.50000000e-02, 3.40000000e-02, 8.23076923e-03, 1.46401853e+00, -5.22375992e-03, 7.56803574e-03, 8.48530000e-03, 6.71755000e-03, 1.39641061e-03, 4.14024959e-03, 1.47976972e-02, 2.03295000e-02],\n",
    "                            [1.18829268e-01, 1.00000000e-03, 4.00000000e-03, 1.17948718e-03, 1.18657803e-01, -3.33958979e-04, 3.55599268e-04, 1.41420000e-03, 1.41420000e-03, 2.60312573e-11, 6.32455532e-04, 5.32284214e-04, 7.07110000e-04],\n",
    "                            [0.09217073, 0.0085, 0.007, 0.00258974, 0.07952256, -0.00104703, 0.00138337, 0.006364, 0.00466692, 0.00203719, 0.00509166, 0.01307342, 0.021213],\n",
    "                            [0.06936585, 0.0095, 0.015, 0.00394872, 0.06837444, -0.00205373, 0.00205373, 0.0084853, 0.00296984, 0.0030984, 0.00234521, 0.00419839, 0.0017678],\n",
    "                            [5.05807317, 0.049, 0.082, 0.02402564, 5.06327737, -0.01120311, 0.01120311, 0.031113, 0.0239, 0.01338272, 0.01117139, 0.04351642, 0.020506],\n",
    "                            [0.26421951, 0.04, 0.068, 0.00902564, 0.2587529, -0.01040894, 0.01040894, 0.025456, 0.01060666, 0.00890233, 0.01111643, 0.04563416, 0.011314],\n",
    "                            [3.59336585, 0.0575, 0.054, 0.02094872, 3.58195886, -0.01804095, 0.01838506, 0.043134, 0.0336584, 0.01240579, 0.01683523, 0.04717173, 0.038184],\n",
    "                            [1.29187805, 0.026, 0.016, 0.00689744, 1.27916244, -0.00322078, 0.00490015, 0.025456, 0.01032378, 0.00861112, 0.01863263, 0.0636921, 0.038537],\n",
    "                            [6.28670732, 0.1245, 0.127, 0.03102564, 6.35501978, -0.01747513, 0.02813757, 0.084146, 0.04690465, 0.0254467, 0.06541464, 0.18275149, 0.15008],\n",
    "                            [10.64578049, 0.079, 0.284, 0.04564103, 10.64447668, -0.01946271, 0.01947497, 0.10889, 0.04186, 0.05739752, 0.06891299, 0.05417812, 0.050205],\n",
    "                            [3.32470732, 0.092, 0.046, 0.01687179, 3.32977984, -0.02794509, 0.02794509, 0.072125, 0.0288498, 0.02428699, 0.06277798, 0.10343739, 0.061518],\n",
    "                            [0.07358537, 0.001, 0.004, 0.00153846, 0.0735262, -0.00027514, 0.00027514, 0.0014142, 0.0009428, 0.00073029, 0.00075277, 0.00053228, 0.00070711]])\n",
    "\n",
    "    return {'dual_coef': dual_coef,\n",
    "            'support_vec': support_vec,\n",
    "            'intercept': intercept,\n",
    "            'gamma': gamma}\n",
    "\n",
    "def interpolateDataTo8Hz(data,sample_rate): #interpolateDataTo8Hz(dict_data['eda'], (1/dict_data['eda']['sampRate'].iloc[0]))\n",
    "    if sample_rate<8:\n",
    "        # Upsample by linear interpolation\n",
    "        data = data.resample(\"125L\").mean()\n",
    "    else:\n",
    "        if sample_rate>8:\n",
    "            # Downsample\n",
    "            idx_range = list(range(0,len(data))) # TODO: double check this one\n",
    "            data = data.iloc[idx_range[0::int(int(sample_rate)/8)]]\n",
    "        # Set the index to be 8Hz\n",
    "        data.index = pd.timedelta_range(start='0S', periods=len(data), freq='125L')\n",
    "        #return data - RETURN STATEMENT 3 - ARRANGE THE APPROPRIATE RETURN STATMENT AT EDA_ARTIFACT_DETECTION_NOTAG AND PREPRO\n",
    "\n",
    "    # Interpolate all empty values\n",
    "    data = data.interpolate()\n",
    "    return data #- RETURN STATEMENT 4 - ARRANGE THE APPROPRIATE RETURN STATMENT AT EDA_ARTIFACT_DETECTION_NOTAG AND PREPRO\n",
    "\n",
    "def butter_lowpass(cutoff, fs, order=5):\n",
    "    # Filtering Helper functions\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = scisig.butter(order, normal_cutoff, btype='low', analog=False) #therefore digital filter\n",
    "    return b, a\n",
    "\n",
    "def butter_lowpass_filter(data, cutoff, fs, order=5):\n",
    "    # Filtering Helper functions\n",
    "    b, a = butter_lowpass(cutoff, fs, order=order)\n",
    "    y = scisig.lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "def getWaveletData(data):\n",
    "    '''\n",
    "    This function computes the wavelet coefficients\n",
    "\n",
    "    INPUT:\n",
    "        data:           DataFrame, index is a list of timestamps at 8Hz, columns include EDA, filtered_eda\n",
    "\n",
    "    OUTPUT:\n",
    "        wave1Second:    DateFrame, index is a list of timestamps at 1Hz, columns include OneSecond_feature1, OneSecond_feature2, OneSecond_feature3 \n",
    "        waveHalfSecond: DateFrame, index is a list of timestamps at 2Hz, columns include HalfSecond_feature1, HalfSecond_feature2 \n",
    "    '''\n",
    "    startTime = data.index[0]\n",
    "    print('from getWaveletData, starttime is: ', startTime, type(startTime)) #NOT RETURN STATEMENT BUT LOOK OUT\n",
    "\n",
    "    # Create wavelet dataframes\n",
    "    oneSecond = pd.date_range(start=startTime, periods=len(data), freq='1s')\n",
    "    halfSecond = pd.date_range(start=startTime, periods=len(data), freq='500L')\n",
    "    #oneSecond = pd.timedelta_range(start=startTime, periods=len(data), freq='1s') #OPTIONAL (NOT) RETURN STATEMENTS TO REPRODUCE ERRORS IF REQUIRED\n",
    "    #halfSecond = pd.timedelta_range(start=startTime, periods=len(data), freq='500L')\n",
    "\n",
    "    # Compute wavelets\n",
    "    cA_n, cD_3, cD_2, cD_1 = pywt.wavedec(data['eda'], 'Haar', level=3) #3 = 1Hz, 2 = 2Hz, 1=4Hz\n",
    "    \n",
    "    # Wavelet 1 second window\n",
    "    N = int(len(data)/8)\n",
    "    coeff1 = np.max(abs(np.reshape(cD_1[0:4*N],(N,4))), axis=1)\n",
    "    coeff2 = np.max(abs(np.reshape(cD_2[0:2*N],(N,2))), axis=1)\n",
    "    coeff3 = abs(cD_3[0:N])\n",
    "    wave1Second = pd.DataFrame({'OneSecond_feature1':coeff1,'OneSecond_feature2':coeff2,'OneSecond_feature3':coeff3})\n",
    "    wave1Second.index = oneSecond[:len(wave1Second)]\n",
    "    \n",
    "    # Wavelet Half second window\n",
    "    N = int(np.floor((len(data)/8.0)*2))\n",
    "    coeff1 = np.max(abs(np.reshape(cD_1[0:2*N],(N,2))),axis=1)\n",
    "    coeff2 = abs(cD_2[0:N])\n",
    "    waveHalfSecond = pd.DataFrame({'HalfSecond_feature1':coeff1,'HalfSecond_feature2':coeff2})\n",
    "    waveHalfSecond.index = halfSecond[:len(waveHalfSecond)]\n",
    "\n",
    "    return wave1Second,waveHalfSecond\n",
    "\n",
    "\n",
    "def getDerivatives(eda):\n",
    "    deriv = (eda[1:-1] + eda[2:])/ 2. - (eda[1:-1] + eda[:-2])/ 2.\n",
    "    second_deriv = eda[2:] - 2*eda[1:-1] + eda[:-2]\n",
    "    #put print statements under these\n",
    "    return deriv,second_deriv\n",
    "\n",
    "def getDerivStats(eda):\n",
    "    deriv, second_deriv = getDerivatives(eda)\n",
    "    maxd = max(deriv)\n",
    "    mind = min(deriv)\n",
    "    maxabsd = max(abs(deriv))\n",
    "    avgabsd = np.mean(abs(deriv))\n",
    "    max2d = max(second_deriv)\n",
    "    min2d = min(second_deriv)\n",
    "    maxabs2d = max(abs(second_deriv))\n",
    "    avgabs2d = np.mean(abs(second_deriv))\n",
    "    \n",
    "    return maxd,mind,maxabsd,avgabsd,max2d,min2d,maxabs2d,avgabs2d\n",
    "\n",
    "\n",
    "def getStats(data):\n",
    "    eda = data['eda'].values\n",
    "    filt = data['filtered_eda'].values\n",
    "    maxd,mind,maxabsd,avgabsd,max2d,min2d,maxabs2d,avgabs2d = getDerivStats(eda)\n",
    "    maxd_f,mind_f,maxabsd_f,avgabsd_f,max2d_f,min2d_f,maxabs2d_f,avgabs2d_f = getDerivStats(filt)\n",
    "    amp = np.mean(eda)\n",
    "    amp_f = np.mean(filt)\n",
    "    return amp, maxd,mind,maxabsd,avgabsd,max2d,min2d,maxabs2d,avgabs2d,amp_f,maxd_f,mind_f,maxabsd_f,avgabsd_f,max2d_f,min2d_f,maxabs2d_f,avgabs2d_f\n",
    "\n",
    "\n",
    "def computeWaveletFeatures(waveDF):\n",
    "    maxList = waveDF.max().tolist()\n",
    "    meanList = waveDF.mean().tolist()\n",
    "    stdList = waveDF.std().tolist()\n",
    "    medianList = waveDF.median().tolist()\n",
    "    aboveZeroList = (waveDF[waveDF>0]).count().tolist()\n",
    "\n",
    "    return maxList,meanList,stdList,medianList,aboveZeroList\n",
    "\n",
    "\n",
    "def getWavelet(wave1Second,waveHalfSecond):\n",
    "    max_1,mean_1,std_1,median_1,aboveZero_1 = computeWaveletFeatures(wave1Second)\n",
    "    max_H,mean_H,std_H,median_H,aboveZero_H = computeWaveletFeatures(waveHalfSecond)\n",
    "    return max_1,mean_1,std_1,median_1,aboveZero_1,max_H,mean_H,std_H,median_H,aboveZero_H\n",
    "\n",
    "\n",
    "def getFeatures(data,w1,wH):\n",
    "    # Get DerivStats\n",
    "    amp,maxd,mind,maxabsd,avgabsd,max2d,min2d,maxabs2d,avgabs2d,amp_f,maxd_f,mind_f,maxabsd_f,avgabsd_f,max2d_f,min2d_f,maxabs2d_f,avgabs2d_f = getStats(data)\n",
    "    statFeat = np.hstack([amp,maxd,mind,maxabsd,avgabsd,max2d,min2d,maxabs2d,avgabs2d,amp_f,maxd_f,mind_f,maxabsd_f,avgabsd_f,max2d_f,min2d_f,maxabs2d_f,avgabs2d_f])\n",
    "\n",
    "    # Get Wavelet Features\n",
    "    max_1,mean_1,std_1,median_1,aboveZero_1,max_H,mean_H,std_H,median_H,aboveZero_H = getWavelet(w1,wH)\n",
    "    waveletFeat = np.hstack([max_1,mean_1,std_1,median_1,aboveZero_1,max_H,mean_H,std_H,median_H,aboveZero_H])\n",
    "\n",
    "    all_feat = np.hstack([statFeat,waveletFeat])\n",
    "    \n",
    "    if np.Inf in all_feat:\n",
    "        print(\"Inf\")\n",
    "    \n",
    "    if np.NaN in all_feat:\n",
    "        print(\"NaN\")\n",
    "\n",
    "    return list(all_feat)\n",
    "\n",
    "\n",
    "def createFeatureDF(data):\n",
    "    '''\n",
    "    INPUTS:\n",
    "        filepath:           string, path to input file  \n",
    "    OUTPUTS:\n",
    "        features:           DataFrame, index is a list of timestamps for each 5 seconds, contains all the features\n",
    "        data:               DataFrame, index is a list of timestamps at 8Hz, columns include eda, filtered_eda\n",
    "    '''\n",
    "    # Load data from q sensor\n",
    "    wave1sec,waveHalf = getWaveletData(data)\n",
    "    \n",
    "    # Create 5 second timestamp list\n",
    "    timestampList = data.index.tolist()[0::40]\n",
    "    \n",
    "    # feature names for DataFrame columns\n",
    "    allFeatureNames = ['raw_amp','raw_maxd','raw_mind','raw_maxabsd','raw_avgabsd','raw_max2d','raw_min2d','raw_maxabs2d','raw_avgabs2d','filt_amp','filt_maxd','filt_mind',\n",
    "        'filt_maxabsd','filt_avgabsd','filt_max2d','filt_min2d','filt_maxabs2d','filt_avgabs2d','max_1s_1','max_1s_2','max_1s_3','mean_1s_1','mean_1s_2','mean_1s_3',\n",
    "        'std_1s_1','std_1s_2','std_1s_3','median_1s_1','median_1s_2','median_1s_3','aboveZero_1s_1','aboveZero_1s_2','aboveZero_1s_3','max_Hs_1','max_Hs_2','mean_Hs_1',\n",
    "        'mean_Hs_2','std_Hs_1','std_Hs_2','median_Hs_1','median_Hs_2','aboveZero_Hs_1','aboveZero_Hs_2']\n",
    "\n",
    "    # Initialize Feature Data Frame\n",
    "    features = pd.DataFrame(np.zeros((len(timestampList),len(allFeatureNames))),columns=allFeatureNames,index=timestampList)\n",
    "    \n",
    "    # Compute features for each 5 second epoch\n",
    "    for i in range(len(features)-1):\n",
    "        start = features.index[i]\n",
    "        end = features.index[i+1]\n",
    "        this_data = data[start:end]\n",
    "        this_w1 = wave1sec[start:end]\n",
    "        this_w2 = waveHalf[start:end]\n",
    "        features.iloc[i] = getFeatures(this_data,this_w1,this_w2)\n",
    "    return features\n",
    "\n",
    "\n",
    "def classifyEpochs(features,featureNames,classifierName):\n",
    "    '''\n",
    "    This function takes the full features DataFrame and classifies each 5 second epoch into artifact, questionable, or clean\n",
    "\n",
    "    INPUTS:\n",
    "        features:           DataFrame, index is a list of timestamps for each 5 seconds, contains all the features\n",
    "        featureNames:       list of Strings, subset of feature names needed for classification\n",
    "        classifierName:     string, type of SVM (binary or multiclass)\n",
    "\n",
    "    OUTPUTS:\n",
    "        labels:             Series, index is a list of timestamps for each 5 seconds, values of -1, 0, or 1 for artifact, questionable, or clean\n",
    "    '''\n",
    "    # Only get relevant features\n",
    "    features = features[featureNames]\n",
    "    X = features[featureNames].values\n",
    "\n",
    "    # Classify each 5 second epoch and put into DataFrame\n",
    "    featuresLabels = predict_binary_classifier(X)\n",
    "    \n",
    "    return featuresLabels\n",
    "\n",
    "\n",
    "def getSVMFeatures(key):\n",
    "    '''\n",
    "    This returns the list of relevant features\n",
    "\n",
    "    INPUT:\n",
    "        key:                string, either \"Binary\" or \"Multiclass\"\n",
    "\n",
    "    OUTPUT:\n",
    "        featureList:        list of Strings, subset of feature names needed for classification\n",
    "    '''\n",
    "    if key == \"Binary\":\n",
    "        return ['raw_amp','raw_maxabsd','raw_max2d','raw_avgabs2d','filt_amp','filt_min2d','filt_maxabs2d','max_1s_1',\n",
    "                                'mean_1s_1','std_1s_1','std_1s_2','std_1s_3','median_1s_3']\n",
    "    elif key == \"Multiclass\":\n",
    "        return ['filt_maxabs2d','filt_min2d','std_1s_1','raw_max2d','raw_amp','max_1s_1','raw_maxabs2d','raw_avgabs2d',\n",
    "                                    'filt_max2d','filt_amp']\n",
    "    else:\n",
    "        print('Error!! Invalid key, choose \"Binary\" or \"Multiclass\"\\n\\n')\n",
    "        return\n",
    "\n",
    "\n",
    "def classify(data):\n",
    "    '''\n",
    "    This function wraps other functions in order to load, classify, and return the label for each 5 second epoch of Q sensor data.\n",
    "\n",
    "    INPUT:\n",
    "        data\n",
    "    OUTPUT:\n",
    "        featureLabels:          Series, index is a list of timestamps for each 5 seconds, values of -1, 0, or 1 for artifact, questionable, or clean\n",
    "        data:                   DataFrame, only output if fullFeatureOutput=1, index is a list of timestamps at 8Hz, columns include eda, filtered_eda\n",
    "    '''\n",
    "    \n",
    "    # Get correct feature names for classifier\n",
    "    classifierName = 'Binary'\n",
    "\n",
    "    # Get pickle List and featureNames list\n",
    "    featureNames = getSVMFeatures(classifierName)\n",
    "\n",
    "    # Create the feature array and then apply the classifier    \n",
    "    features = createFeatureDF(data)\n",
    "    labels   = classifyEpochs(features, featureNames, classifierName)\n",
    "\n",
    "    return labels, data\n",
    "\n",
    "def plotData_notag(data, labels, filepath, subject, tag, filteredPlot=0, secondsPlot=0): #maybe change filteredPlot to 1, plotData_notag(data, labels, dir_out, subject, tag, 1, 0)\n",
    "    '''\n",
    "    This function plots the Q sensor EDA data with shading for artifact (red) and questionable data (grey). \n",
    "        Note that questionable data will only appear if you choose a multiclass classifier\n",
    "\n",
    "    INPUT:\n",
    "        data:                   DataFrame, indexed by timestamps at 8Hz, columns include EDA and filtered_eda\n",
    "        labels:                 array, each row is a 5 second period and each column is a different classifier\n",
    "        filteredPlot:           binary, 1 for including filtered EDA in plot, 0 for only raw EDA on the plot, defaults to 0\n",
    "        secondsPlot:            binary, 1 for x-axis in seconds, 0 for x-axis in minutes, defaults to 0\n",
    "\n",
    "    OUTPUT:\n",
    "        [plot]                  the resulting plot has N subplots (where N is the length of classifierList) that have linked x and y axes \n",
    "                                    and have shading for artifact (red) and questionable data (grey)\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # Initialize x axis\n",
    "    if secondsPlot:\n",
    "        scale = 1.0\n",
    "    else:\n",
    "        scale = 60.0\n",
    "    time_m = np.arange(0,len(data))/(8.0*scale)\n",
    "    \n",
    "    # Initialize Figure\n",
    "    plt.figure(figsize=(10,5))\n",
    "\n",
    "    # For each classifier, label each epoch and plot\n",
    "    key = 'Binary'\n",
    "        \n",
    "    # Initialize Subplots\n",
    "    ax = plt.subplot(1,1,1)\n",
    "\n",
    "    # Plot EDA\n",
    "    ax.plot(time_m,data['eda'], c='b', label ='Raw SC')\n",
    "\n",
    "    # For each epoch, shade if necessary\n",
    "    for i in range(0,len(labels)-1):\n",
    "        if labels[i]==-1:\n",
    "            # artifact\n",
    "            start = i*40/(8.0*scale)\n",
    "            end = start+5.0/scale\n",
    "            ax.axvspan(start, end, facecolor='red', alpha=0.7, edgecolor ='none', label = 'artifact') #better to give this a label so that in legend it doesn't keep getting incorrectly labelled as filtered data\n",
    "        elif labels[i]==0:\n",
    "            # Questionable\n",
    "            start = i*40/(8.0*scale)\n",
    "            end = start+5.0/scale\n",
    "            ax.axvspan(start, end, facecolor='.5', alpha=0.5,edgecolor ='none', label = 'questionable')\n",
    "\n",
    "    # Plot filtered data if requested\n",
    "    if filteredPlot:\n",
    "        ax.plot(time_m-.625/scale,data['filtered_eda'], c='g', linestyle='--', label = 'Filtered SC') #maybe make this dashed line so that the underlying raw data in blue can still be seen because now the raw data is being completely covered.\n",
    "        plt.legend(['Raw SC','Filtered SC', 'Artifact'],loc=0)\n",
    "    else:\n",
    "        plt.legend(['Raw SC', 'Artifact'],loc=0)\n",
    "\n",
    "    # Label and Title each subplot\n",
    "    plt.ylabel('$\\mu$S')\n",
    "    plt.title(key)\n",
    "    \n",
    "    # Only include x axis label on final subplot\n",
    "    if secondsPlot:\n",
    "        plt.xlabel('Time (s)')    \n",
    "    else:\n",
    "        plt.xlabel('Time (min)')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.subplots_adjust(hspace=.3)\n",
    "    plt.show()\n",
    "    plt.savefig(os.path.join(filepath, subject + '_' + tag + '_artefacts.png'), dpi = 300) #take out the exp after experimenting is done\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "\n",
    "def EDA_artifact_detection_notag(dict_data, dir_out, subject, tag):\n",
    "    \n",
    "    # make sure data has 8Hz\n",
    "    data = interpolateDataTo8Hz(dict_data['eda'], (1/dict_data['eda']['sampRate'].iloc[0])) #interpolateDataTo8Hz(dict_df['eda'], (1/dict_df['sampRate'].iloc[0]))#data  = interpolateDataTo8Hz(dict_df['eda'], (1/dict_df['eda']['sampRate'].iloc[0]))\n",
    "    \n",
    "    # forward propagate data to fill NAs after merging\n",
    "    data = data.ffill()\n",
    "    #return data\n",
    "    \n",
    "    \n",
    "    # get the filtered data using a low-pass butterworth filter (cutoff:1hz, fs:8hz, order:6)\n",
    "    data['filtered_eda'] =  butter_lowpass_filter(data['eda'], 1.0, 8, 6) #butter_lowpass_filter(data, 1.0, 8, 6) #butter_lowpass_filter(data['eda'], 1.0, 8, 6)\n",
    "    #return data - RETURN STATEMENT 5 (TO SEE WHAT THE FILTERED EDA COLUMN LOOKS LIKE) - ARRANGE RETURNS IN OUTER FUNCTIONS ACCORDINGLY    \n",
    "    \n",
    "    \n",
    "    # classify the data\n",
    "    labels, data = classify(data)\n",
    "\n",
    "    # plot data\n",
    "    plotData_notag(data, labels, dir_out, subject, tag, 1, 0)   #change the second last entry back to 1 after experimenting is done - changed back\n",
    "\n",
    "    # save labels\n",
    "    fullOutputPath = os.path.join(dir_out, subject + '_' + tag + '_artefacts.csv')\n",
    "\n",
    "    #featureLabels = pd.DataFrame(labels, index=pd.timedelta_range(start=data.index[0], periods=len(labels), freq='5s'),\n",
    "                                 #columns=['Binary'])\n",
    "    featureLabels = pd.DataFrame(labels, index=pd.date_range(start=data.index[0], periods=len(labels), freq='5s'),\n",
    "                                 columns=['Binary'])\n",
    "\n",
    "    featureLabels.reset_index(inplace=True)\n",
    "    featureLabels.rename(columns={'index':'StartTime'}, inplace=True)\n",
    "    #featureLabels['EndTime'] = featureLabels['StartTime']+datetime.timedelta(seconds=5)\n",
    "    featureLabels['EndTime'] = featureLabels['StartTime']+timedelta(seconds=5)\n",
    "    featureLabels.index.name = 'EpochNum'\n",
    "\n",
    "    featureLabels.to_csv(fullOutputPath)\n",
    "    \n",
    "    return featureLabels\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "This is a preprocessing pipeline for psychophysiological data collected with \n",
    "Empatica wristbands, either E4 or E+. It uses EDA Explorer to detect \n",
    "artefacts in the data based on EDA, temperature and acceleration data using\n",
    "a binary classifier (noise versus okay). For the preprocessing, it uses \n",
    "functions from the NeuroKit2 package. Specifically for the EDA preprocessing, \n",
    "it also draws inspiration from Ledalab. \n",
    "\n",
    "Arguments: \n",
    "    empatica   : either 'e4' or 'e+' or 'cut' (4Hz EDA and 64Hz BVP only)\n",
    "    winwidth   : width of the window for smoothing of EDA with Gaussian kernel (int)\n",
    "    lowpass    : lowpass filter frequency for EDA - has to be no larger than half the sample rate\n",
    "    dir_out    : output directory for all the results\n",
    "    dir_path   : input directory\n",
    "    exclude    : list of patterns to be excluded from preprocessing\n",
    "\n",
    "The function creates preprocessed data files for EDA and BVP as well as plots \n",
    "to check the data quality. \n",
    "\n",
    "(c) Irene Sophia Plank, 10planki@gmail.com\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# load all modules\n",
    "import neurokit2 as nk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import simple_colors\n",
    "import math\n",
    "import glob\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "#warnings.simplefilter('ignore', UserWarning)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from avro.datafile import DataFileReader\n",
    "from avro.io import DatumReader\n",
    "from datetime import datetime\n",
    "\n",
    "#added by me:\n",
    "from datetime import timedelta\n",
    "import pytz\n",
    "from pathlib import Path\n",
    "#from EDA_artifactdetection_short import EDA_artifact_detection\n",
    "\n",
    "###### Helper functions\n",
    "\n",
    "def gauss_smoothing(data, winwidth):\n",
    "    # Gaussian smoothing of EDA data\n",
    "    \n",
    "    # pad to remove border errors\n",
    "    fdata = pd.concat([pd.Series(data.iloc[0]), data, pd.Series(data.iloc[-1])])\n",
    "    \n",
    "    # ensure an even window width\n",
    "    winwidth = math.floor(winwidth/2)*2\n",
    "    \n",
    "    # extend data to reduce convolution error at beginning and end\n",
    "    data_ext = pd.concat([pd.Series([fdata.iloc[0]]*int(winwidth/2)), fdata, pd.Series([fdata.iloc[-1]]*int(winwidth/2))])\n",
    "    \n",
    "    # apply normpdf (?)\n",
    "    x = np.array(range(1, winwidth+2))\n",
    "    mu = winwidth/2+1\n",
    "    sigma = winwidth/8\n",
    "    window = np.exp(-0.5 * ((x - mu)/sigma)**2) / (math.sqrt(2*math.pi) * sigma)\n",
    "    window = window / sum(window)\n",
    "    \n",
    "    # perform convolution for smoothing\n",
    "    sdata_ext = np.convolve(data_ext, window)\n",
    "    \n",
    "    # cut to length of data\n",
    "    return sdata_ext[(1+winwidth):(len(sdata_ext)-winwidth-1)]\n",
    "\n",
    "def int_missing(df_eda, df_bvp, df_temp, df_acc, f):\n",
    "    # performing linear interpolation for bvp, temp and acc as well as cubic\n",
    "    # spline interpolation for eda\n",
    "    \n",
    "    # only process temp if it is not empty\n",
    "    if len(df_temp) > 0:\n",
    "        sampRate = df_temp['sampRate'].iloc[0]\n",
    "        df_temp = df_temp.interpolate()\n",
    "        df_temp = df_temp.bfill()\n",
    "        df_temp['sampRate'] = sampRate\n",
    "    \n",
    "    # only process acc if it is not empty\n",
    "    if len(df_acc) > 0:\n",
    "        sampRate = df_acc['sampRate'].iloc[0]\n",
    "        df_acc = df_acc.interpolate()\n",
    "        df_acc = df_acc.bfill()\n",
    "        df_acc['sampRate'] = sampRate\n",
    "    \n",
    "    # EDA\n",
    "    #sampRate = df_eda['sampRate'].iloc[0]  - commented out in this line and added below because the first sampRate value was turning up as NaN\n",
    "    raw    = df_eda['eda']\n",
    "    df_eda = df_eda.interpolate(method='spline', order=3)\n",
    "    df_eda = df_eda.bfill()\n",
    "    df_eda['raw'] = raw\n",
    "    sampRate = df_eda['sampRate'].iloc[0]\n",
    "    df_eda['sampRate'] = sampRate\n",
    "    \n",
    "    # BVP\n",
    "    sampRate = df_bvp['sampRate'].iloc[0]\n",
    "    raw    = df_bvp['bvp']\n",
    "    df_bvp = df_bvp.interpolate()\n",
    "    df_bvp = df_bvp.bfill()\n",
    "    df_bvp['raw'] = raw\n",
    "    df_bvp['sampRate'] = sampRate\n",
    "    \n",
    "    # print how much was interpolated for bvp and eda\n",
    "    per_eda = np.mean(raw.isna())\n",
    "    per_bvp = np.mean(raw.isna())\n",
    "    if round(per_eda, 2) == round(per_bvp, 2):\n",
    "        if per_eda >= 0.2:\n",
    "            print(simple_colors.red(datetime.now().strftime(\"%H:%M:%S\") + ' - ' + str(round(per_eda*100,2)) + ' percent of data were interpolated', 'bold'))\n",
    "        elif per_eda >= 0.01:\n",
    "            print(simple_colors.yellow(datetime.now().strftime(\"%H:%M:%S\") + ' - ' + str(round(per_eda*100,2)) + ' percent of data were interpolated', 'bold'))\n",
    "    else:\n",
    "        if per_eda >= 0.2:\n",
    "            print(simple_colors.red(datetime.now().strftime(\"%H:%M:%S\") + ' - ' + str(round(per_eda*100,2)) + ' percent EDA were interpolated', 'bold'))\n",
    "        elif per_eda >= 0.01:\n",
    "            print(simple_colors.yellow(datetime.now().strftime(\"%H:%M:%S\") + ' - ' + str(round(per_eda*100,2)) + ' percent EDA were interpolated', 'bold'))\n",
    "        if per_bvp >= 0.2:\n",
    "            print(simple_colors.red(datetime.now().strftime(\"%H:%M:%S\") + ' - ' + str(round(per_bvp*100,2)) + ' percent BVP were interpolated', 'bold'))\n",
    "        elif per_bvp >= 0.01:\n",
    "            print(simple_colors.yellow(datetime.now().strftime(\"%H:%M:%S\") + ' - ' + str(round(per_bvp*100,2)) + ' percent BVP were interpolated', 'bold'))\n",
    "    \n",
    "    # write to log file\n",
    "    f.write('\\n' + datetime.now().strftime(\"%H:%M:%S\") + ' - ' + str(round(per_bvp*100,2)) + '% BVP and ' + str(round(per_eda*100,2)) + '% EDA were interpolated')\n",
    "    \n",
    "    # return the dataframes\n",
    "    return df_eda, df_bvp, df_temp, df_acc\n",
    "\n",
    "def na_missing(df_eda, df_bvp, labels):\n",
    "    # taking the labels from the artefact detection classifier and create column\n",
    "    # with NaNs where artefacts were detected\n",
    "    \n",
    "    # join label dataframes with the dataframes containing EDA and BVP\n",
    "    labels.index = labels['StartTime']\n",
    "    df_eda = df_eda.join(labels, how='outer')\n",
    "    df_bvp = df_bvp.join(labels, how='outer')\n",
    "    \n",
    "    # fill up the NaNs with the preceeding label\n",
    "    df_eda['Binary'] = df_eda['Binary'].fillna(method='ffill')\n",
    "    df_bvp['Binary'] = df_bvp['Binary'].fillna(method='ffill')\n",
    "    \n",
    "    # calculate the new column and drop the unnecessary ones\n",
    "    df_eda['eda'] = np.where(df_eda['Binary'] == 1, df_eda['eda'], np.nan)\n",
    "    df_bvp['bvp'] = np.where(df_bvp['Binary'] == 1, df_bvp['bvp'], np.nan)\n",
    "    df_eda = df_eda.drop(columns=['StartTime', 'EndTime'])\n",
    "    df_bvp = df_bvp.drop(columns=['StartTime', 'EndTime'])\n",
    "    \n",
    "    return df_eda, df_bvp\n",
    "\n",
    "###### Cut and convert the data\n",
    "\n",
    "def read_avro(filepath, timezone = 'Europe/Berlin'):\n",
    "    # reading in avro data and converting it into data frames with the correct time stamp\n",
    "    \n",
    "    # read in the avro data\n",
    "    reader = DataFileReader(open(filepath, \"rb\"), DatumReader())\n",
    "    for user in reader:\n",
    "        dict_data = user\n",
    "    reader.close()\n",
    "\n",
    "    target_timezone = pytz.timezone(timezone) #change to required timezone if needed - done\n",
    "    # temperature: \n",
    "    startTime = datetime.utcfromtimestamp((float(dict_data['rawData']['temperature']['timestampStart'])/(10**(len(str(dict_data['rawData']['temperature']['timestampStart']))-10)))).replace(tzinfo=pytz.utc).astimezone(target_timezone) #1000000\n",
    "    #cet = pytz.timezone('Europe/Berlin') #me\n",
    "    #startTime = cet.localize(startTime) #me\n",
    "    sampRate  = dict_data['rawData']['temperature']['samplingFrequency']\n",
    "    df_temp   = pd.DataFrame(dict_data['rawData']['temperature']['values'], columns=[\"temp\"])\n",
    "    if sampRate > 0.0:\n",
    "        freq      = str(round(1/sampRate)) + 'S'\n",
    "        time      = pd.date_range(startTime, periods=len(df_temp), freq=freq)\n",
    "        df_temp.index = time\n",
    "        df_temp['sampRate'] = round(1/sampRate)\n",
    "    \n",
    "    # acceleration\n",
    "    startTime = datetime.utcfromtimestamp((float(dict_data['rawData']['accelerometer']['timestampStart'])/(10**(len(str(dict_data['rawData']['accelerometer']['timestampStart']))-10)))).replace(tzinfo=pytz.utc).astimezone(target_timezone)\n",
    "    #cet = pytz.timezone('Europe/Berlin') #me\n",
    "    #startTime = cet.localize(startTime) #me\n",
    "    sampRate  = dict_data['rawData']['accelerometer']['samplingFrequency']\n",
    "    df_acc    = pd.DataFrame({'accx_raw': dict_data['rawData']['accelerometer']['x'],\n",
    "                              'accy_raw': dict_data['rawData']['accelerometer']['y'],\n",
    "                              'accz_raw': dict_data['rawData']['accelerometer']['z']})\n",
    "    if sampRate > 0.0:\n",
    "        freq      = str(round(1/sampRate, 6)) + 'S'\n",
    "        time      = pd.date_range(startTime, periods=len(df_acc), freq=freq)\n",
    "        df_acc.index = time\n",
    "        df_acc['sampRate'] = round(1/sampRate, 6)\n",
    "    \n",
    "    # bvp\n",
    "    startTime = datetime.utcfromtimestamp((float(dict_data['rawData']['bvp']['timestampStart'])/(10**(len(str(dict_data['rawData']['bvp']['timestampStart']))-10)))).replace(tzinfo=pytz.utc).astimezone(target_timezone)\n",
    "    #cet = pytz.timezone('Europe/Berlin') #me\n",
    "    #startTime = cet.localize(startTime) #me\n",
    "    sampRate  = dict_data['rawData']['bvp']['samplingFrequency']\n",
    "    df_bvp    = pd.DataFrame({'bvp': dict_data['rawData']['bvp']['values']})\n",
    "    if sampRate > 0.0:\n",
    "        freq      = str(round(1/sampRate, 6)) + 'S'\n",
    "        time      = pd.date_range(startTime, periods=len(df_bvp), freq=freq)\n",
    "        df_bvp.index = time\n",
    "        df_bvp['sampRate'] = round(1/sampRate, 6)\n",
    "        \n",
    "    # eda\n",
    "    startTime = datetime.utcfromtimestamp((float(dict_data['rawData']['eda']['timestampStart'])/(10**(len(str(dict_data['rawData']['eda']['timestampStart']))-10)))).replace(tzinfo=pytz.utc).astimezone(target_timezone)\n",
    "    #print(\"start time using old utc_cet conversion method \", startTime)\n",
    "    #cet = pytz.timezone('Europe/Berlin') #me\n",
    "    #startTime = cet.localize(startTime) #me this stuff doesn't work so commented out\n",
    "    #print(\"start time after attempted another attempted cet conversion \", startTime)\n",
    "    sampRate  = dict_data['rawData']['eda']['samplingFrequency']\n",
    "    df_eda    = pd.DataFrame({'eda': dict_data['rawData']['eda']['values']})\n",
    "    if sampRate > 0.0:\n",
    "        freq      = str(round(1/sampRate, 2)) + 'S'\n",
    "        time      = pd.date_range(startTime, periods=len(df_eda), freq=freq)\n",
    "        df_eda.index = time\n",
    "        df_eda['sampRate'] = round(1/sampRate, 2)\n",
    "    \n",
    "    # return all data frames\n",
    "    return df_temp, df_acc, df_bvp, df_eda\n",
    "\n",
    "\n",
    "def convert_eplus_notag(dir_path, f, timezone):\n",
    "    # reading in and converting data collected with Embrace Plus\n",
    "    \n",
    "    # get list of all files of this participant\n",
    "    #fls = glob.glob(os.path.join(dir_path, part + '*.avro')) #glob.glob(os.path.join(dir_path, 'participant_data',\n",
    "                                 #'*', '*', 'raw_data', 'v*', part + '*.avro'))\n",
    "    fls = glob.glob(os.path.join(dir_path, '*.avro')) #glob.glob(os.path.join(dir_path, 'participant_data',\n",
    "                                 #'*', '*', 'raw_data', 'v*', part + '*.avro'))\n",
    "    \n",
    "    # check if any data was found\n",
    "    if len(fls) < 1:\n",
    "        print(simple_colors.red(datetime.now().strftime(\"%H:%M:%S\") + '- no data was found for participant ' + part, 'bold'))\n",
    "        f.write('\\n' + datetime.now().strftime(\"%H:%M:%S\") + '- no data was found for participant ' + part)\n",
    "        return {}\n",
    "    \n",
    "    # sort by start time in unix\n",
    "    fls = sorted(fls, key=lambda i: int(os.path.splitext(os.path.basename(i))[0][-9:]))\n",
    "    \n",
    "    # initialise empty data frames\n",
    "    ls_temp = list()\n",
    "    ls_acc  = list()\n",
    "    ls_eda  = list()\n",
    "    ls_bvp  = list()\n",
    "    \n",
    "    # loop through files\n",
    "    for fl in fls:\n",
    "        # read in the avro data\n",
    "        df_temp, df_acc, df_bvp, df_eda = read_avro(fl, timezone)\n",
    "        # check timing difference: \n",
    "        if len(ls_temp) > 0 & len(df_temp) > 0: \n",
    "            # temperature: 1Hz -> should be about 1 per second\n",
    "            idx_temp = pd.date_range(ls_temp[-1].index[-1]+pd.Timedelta(seconds=df_temp['sampRate'].iloc[0]), \n",
    "                                     df_temp.index[0]-pd.Timedelta(seconds=df_temp['sampRate'].iloc[0]), freq=str(df_temp['sampRate'].iloc[0])+'S')\n",
    "            \"\"\"\n",
    "            start_time_temp = ls_temp[-1].index[-1]+pd.Timedelta(seconds=df_temp['sampRate'].iloc[0])\n",
    "            end_time_temp = df_temp.index[0]-pd.Timedelta(seconds=df_temp['sampRate'].iloc[0])\n",
    "            #making them time-zone aware\n",
    "            start_time_temp = start_time_temp.tz_localize('Europe/Berlin') if start_time_temp.tzinfo is None else start_time_temp\n",
    "            end_time_temp = end_time_temp.tz_localize('Europe/Berlin') if end_time_temp.tzinfo is None else end_time_temp\n",
    "            \n",
    "            idx_temp = pd.date_range(start_time_temp, \n",
    "                                     end_time_temp, freq=str(df_temp['sampRate'].iloc[0])+'S', tz = 'Europe/Berlin') #me\n",
    "            \"\"\"\n",
    "            tdf_temp = pd.DataFrame(np.nan, index=idx_temp, columns=['temp'])\n",
    "            df_temp  = pd.concat([tdf_temp, df_temp])\n",
    "        \n",
    "        if len(ls_acc) > 0 & len(df_acc) > 0: \n",
    "            # acceleration: 64Hz -> should be about 1 every 15.625ms\n",
    "            idx_acc  = pd.date_range(ls_acc[-1].index[-1]+pd.Timedelta(seconds=df_acc['sampRate'].iloc[0]), \n",
    "                                     df_acc.index[0]-pd.Timedelta(seconds=df_acc['sampRate'].iloc[0]), freq=str(df_acc['sampRate'].iloc[0])+'S')\n",
    "            \"\"\"\n",
    "            start_time_acc = ls_acc[-1].index[-1]+pd.Timedelta(seconds=df_acc['sampRate'].iloc[0])\n",
    "            end_time_acc = df_acc.index[0]-pd.Timedelta(seconds=df_acc['sampRate'].iloc[0])\n",
    "            #making them time-zone aware\n",
    "            start_time_acc = start_time_acc.tz_localize('Europe/Berlin') if start_time_acc.tzinfo is None else start_time_acc\n",
    "            end_time_acc = end_time_acc.tz_localize('Europe/Berlin') if end_time_acc.tzinfo is None else end_time_acc\n",
    "            \n",
    "            idx_acc  = pd.date_range(start_time_acc, \n",
    "                                     end_time_acc, freq=str(df_acc['sampRate'].iloc[0])+'S', tz = 'Europe/Berlin') #me\n",
    "            \"\"\"\n",
    "            tdf_acc  = pd.DataFrame(np.nan, index=idx_acc, columns=['accx', 'accy', 'accz'])\n",
    "            df_acc   = pd.concat([tdf_acc, df_acc])\n",
    "        \n",
    "        if len(ls_eda) > 0 & len(df_eda) > 0:     \n",
    "            # eda: 4Hz -> should be about 1 every 250ms\n",
    "            idx_eda  = pd.date_range(ls_eda[-1].index[-1]+pd.Timedelta(seconds=df_eda['sampRate'].iloc[0]), \n",
    "                                     df_eda.index[0]-pd.Timedelta(seconds=df_eda['sampRate'].iloc[0]), freq=str(df_eda['sampRate'].iloc[0])+'S')\n",
    "            \n",
    "            \"\"\"\n",
    "            start_time_eda = ls_eda[-1].index[-1]+pd.Timedelta(seconds=df_eda['sampRate'].iloc[0])\n",
    "            end_time_eda = df_eda.index[0]-pd.Timedelta(seconds=df_eda['sampRate'].iloc[0])\n",
    "            #making them time-zone aware\n",
    "            start_time_eda = start_time_eda.tz_localize('Europe/Berlin') if start_time_eda.tzinfo is None else start_time_eda\n",
    "            end_time_eda = end_time_eda.tz_localize('Europe/Berlin') if end_time_eda.tzinfo is None else end_time_eda\n",
    "            \n",
    "            idx_eda  = pd.date_range(start_time_eda, \n",
    "                                     end_time_eda, freq=str(df_eda['sampRate'].iloc[0])+'S', tz = 'Europe/Berlin') #me\n",
    "            \"\"\"\n",
    "            tdf_eda  = pd.DataFrame(np.nan, index=idx_eda, columns=['eda'])\n",
    "            df_eda   = pd.concat([tdf_eda, df_eda])\n",
    "           \n",
    "        \n",
    "        if len(ls_bvp) > 0 & len(df_bvp) > 0:     \n",
    "            # bvp: 1Hz -> 64Hz -> should be about 1 every 15.625ms\n",
    "            idx_bvp = pd.date_range(ls_bvp[-1].index[-1]+pd.Timedelta(seconds=df_bvp['sampRate'].iloc[0]), \n",
    "                                     df_bvp.index[0]-pd.Timedelta(seconds=df_bvp['sampRate'].iloc[0]), freq=str(df_bvp['sampRate'].iloc[0])+'S')\n",
    "            \"\"\"\n",
    "            start_time_bvp = ls_bvp[-1].index[-1]+pd.Timedelta(seconds=df_bvp['sampRate'].iloc[0])\n",
    "            end_time_bvp = df_bvp.index[0]-pd.Timedelta(seconds=df_bvp['sampRate'].iloc[0])\n",
    "            #making them time-zone aware\n",
    "            start_time_bvp = start_time_bvp.tz_localize('Europe/Berlin') if start_time_bvp.tzinfo is None else start_time_bvp\n",
    "            end_time_bvp = end_time_bvp.tz_localize('Europe/Berlin') if end_time_bvp.tzinfo is None else end_time_bvp\n",
    "            \n",
    "            idx_bvp = pd.date_range(start_time_bvp, \n",
    "                                     end_time_bvp, freq=str(df_bvp['sampRate'].iloc[0])+'S', tz = 'Europe/Berlin') #me\n",
    "            \"\"\"\n",
    "            tdf_bvp  = pd.DataFrame(np.nan, index=idx_bvp, columns=['bvp'])\n",
    "            df_bvp   = pd.concat([tdf_bvp, df_bvp])\n",
    "            \n",
    "        # append to lists\n",
    "        if len(df_bvp) > 0:\n",
    "            ls_bvp.append(df_bvp)\n",
    "        if len(df_temp) > 0:\n",
    "            ls_temp.append(df_temp)\n",
    "        if len(df_acc) > 0:\n",
    "            # scale the accelometer to +-2g: \"each ADC count will be = 1/2048g\" (email from 12.12.2023)\n",
    "            df_acc[\"accx\"] = df_acc[\"accx_raw\"]/2048\n",
    "            df_acc[\"accy\"] = df_acc[\"accy_raw\"]/2048\n",
    "            df_acc[\"accz\"] = df_acc[\"accz_raw\"]/2048\n",
    "            \n",
    "            ls_acc.append(df_acc)\n",
    "        if len(df_eda) > 0:\n",
    "            ls_eda.append(df_eda)\n",
    "             #return tdf_eda, df_eda, ls_eda\n",
    "    #\"\"\"\n",
    "    # concat list of dataframes to dataframes in dictionary\n",
    "    return {\n",
    "        'temp' : pd.concat(ls_temp),\n",
    "        'acc'  : pd.concat(ls_acc),\n",
    "        'bvp'  : pd.concat(ls_bvp),\n",
    "        'eda'  : pd.concat(ls_eda)\n",
    "    }\n",
    "    #\"\"\"\n",
    "\n",
    "\n",
    "###### EDA\n",
    "\n",
    "def eda_prepro_notag(dir_out, df_eda, subject, key, winwidth, lowpass, f, timezone): #(dir_out, df_eda, subject, key, winwidth, [], f) \n",
    "    # preprocessing EDA data\n",
    "\n",
    "    # get sampling rate in Hz\n",
    "    sr = 1/df_eda['sampRate'].iloc[0]\n",
    "    print(\"sr = \", sr)\n",
    "    # if a filter was supposed to be applied\n",
    "    if isinstance(lowpass, int):\n",
    "        # check if digital filter critical frequency 0 < lowpass < fs/2\n",
    "        if (lowpass > 0) & (lowpass < sr/2):\n",
    "            fil = scipy.signal.butter(1, 5, fs = sr)\n",
    "            data = scipy.signal.sosfilt(fil, df_eda['eda'])\n",
    "        else:\n",
    "            data = df_eda['eda']\n",
    "            print(simple_colors.red('No filtering applied.', 'bold'), 'Critical frequency must be at least half the sampling rate for smoothing to be performed.')\n",
    "            f.write('\\n' + 'No filtering applied. Critical frequency must be at least half the sampling rate for smoothing to be performed.')\n",
    "    else:\n",
    "        print(\"no filtering applied\") #added by me\n",
    "        data = df_eda['eda']\n",
    "    \n",
    "    # smooth the data - same as GAUSS option in ledalab\n",
    "    df_eda['eda_smooth'] = gauss_smoothing(data, winwidth)\n",
    "\n",
    "    #return df_eda\n",
    "    \n",
    "    # process the data, including TTG to detect peaks\n",
    "    signals, info = nk.eda_process(df_eda['eda_smooth'], sampling_rate = sr)\n",
    "    #return df_eda, signals, info\n",
    "\n",
    "    \n",
    "    # combine the data frames\n",
    "    signals.index = df_eda.index\n",
    "    df_eda = signals.join(df_eda)\n",
    "    \n",
    "    \n",
    "    df_eda = df_eda.drop(['EDA_Raw', 'eda_smooth'], axis=1).rename(columns={\"eda\": \"EDA_Raw\"}, errors=\"raise\")\n",
    "    signals = df_eda.reset_index()\n",
    "    \n",
    "    # visualise the signals\n",
    "    matplotlib.rcParams['figure.figsize'] = (100, 10)\n",
    "    nk.eda_plot(signals, info)\n",
    "    plt.savefig(os.path.join(dir_out, subject + '_' + key + '_eda_signals.png'), dpi = 300)\n",
    "    \n",
    "    # close all figures\n",
    "    plt.close(\"all\") \n",
    "\n",
    "    #return df_eda, signals, info\n",
    "    \n",
    "    #filepath = the empatica folder for that day\n",
    "    \n",
    "    filepath = Path(dir_path).parent\n",
    "    print(filepath)\n",
    "    df_eda_filtered = additional_filters(df_eda, filepath, timezone) \n",
    "    # save data to csv\n",
    "    df_eda.rename(columns={\"raw\": \"EDA_Raw_noint\"}).to_csv(os.path.join(dir_out, subject + '_' + key + '_eda_signals.csv'), index = True)\n",
    "    info.pop('sampling_rate')\n",
    "    info_df = pd.DataFrame.from_dict(info)\n",
    "    info_df.to_csv(os.path.join(dir_out, subject + '_' + key + '_eda_scr.csv'), index = True)\n",
    "    df_eda_filtered.rename(columns={\"raw\": \"EDA_Raw_noint\"}).to_csv(os.path.join(dir_out, subject + '_' + key + '_eda_signals_filtered.csv'), index = True)\n",
    "    #call back the previously saved fig or re-visualise the eda figure and whereever the data has been turned NA, put red lines just like the artifact plot function\n",
    "    \n",
    "    return df_eda, df_eda_filtered, signals, info\n",
    "    #return\n",
    "    \n",
    "\"\"\"\n",
    "def additional_filters(df_eda, filepath):\n",
    "    #read in temp.csv as a dataframe from preprocessed files folder\n",
    "    #read agg eda.csv as a dataframe from aggr_p_min folder\n",
    "    #add a new column to df_eda called \"temp_data_validity\" -> +1 if within required temperature range and -1 if not; match by timestamp\n",
    "    #add a new column to df_eda called \"device_recording_validity\" -> -1 if \"device_not_recording\" or \"device_not_worn_correctly\" and +1 if not; for every\n",
    "    #now for all the rows of df_eda, if the \"temp_data_validity\" or \"device_recording_validity\" columns are -1, all eda columns are made NA\n",
    "    return df_eda\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def additional_filters(df_eda, filepath, timezone):\n",
    "    folder11 = 'aggr_p_min'\n",
    "    folder12 = 'avro_files'\n",
    "    #folder13 = 'avro2csv'\n",
    "    folder14 = 'preprocessed_files_debug'\n",
    "    folder141 = 'data_preproc_debug'\n",
    "    #read in temp.csv as a dataframe from aggr_p_min folder (not preprocessed files folder because it is data for every 1 second and the timestamps do not match. Therefore there is no coding-based advantage to using it. Moreover, a second by second data elimination is not needed. The per minute aggregated data is a better and stricter way of filtering out untrustworthy data because the entire minute's data, if it is not valid, can be eliminated.\n",
    "    for file in os.listdir(os.path.join(filepath, folder11)):\n",
    "        if file.endswith('temperature.csv'):\n",
    "            temp_file = pd.read_csv(os.path.join(filepath, folder11, file))\n",
    "    # Define the time zones\n",
    "    utc_zone = pytz.utc\n",
    "    req_zone = pytz.timezone(timezone)\n",
    "\n",
    "    # Function to convert UTC to Europe/Berlin time\n",
    "    def from_isoutc_to_req(iso_timestamp):\n",
    "        # Parse the ISO 8601 timestamp into a datetime object\n",
    "        utc_time = datetime.fromisoformat(iso_timestamp.replace(\"Z\", \"+00:00\"))\n",
    "    \n",
    "        # Localize the datetime object to UTC\n",
    "        #utc_time = utc_zone.localize(utc_time) check this line and see if it is still required\n",
    "    \n",
    "        # Convert to Berlin time\n",
    "        req_time = utc_time.astimezone(req_zone)\n",
    "        #print(req_time, type(req_time))\n",
    "    \n",
    "        return req_time #.isoformat()\n",
    "\n",
    "    # Apply the conversion function to the 'utc_timestamps' column and create a new column 'converted_timestamps'\n",
    "    temp_file['converted_timestamps'] = temp_file['timestamp_iso'].apply(from_isoutc_to_req)\n",
    "    \n",
    "    #read agg eda.csv as a dataframe from aggr_p_min folder\n",
    "    for file in os.listdir(os.path.join(filepath, folder11)):\n",
    "        if file.endswith('eda.csv'):\n",
    "            eda_aggr_file = pd.read_csv(os.path.join(filepath, folder11, file))\n",
    "\n",
    "    # Apply the conversion function to the 'utc_timestamps' column and create a new column 'converted_timestamps'\n",
    "    eda_aggr_file['converted_timestamps'] = eda_aggr_file['timestamp_iso'].apply(from_isoutc_to_req)\n",
    "\n",
    "    \n",
    "    #add a new column to df_eda called \"temp_data_validity\" -> +1 if within required temperature range and -1 if not; match by timestamp\n",
    "    #tenative min and max values => modify to verified values\n",
    "    temp_min= 30\n",
    "    temp_max = 40\n",
    "    # Create a new column in df_eda initialized with NaN\n",
    "    df_eda['temp_data_validity'] = np.nan\n",
    "\n",
    "    # Create a boolean mask for valid temperatures in temp_file\n",
    "    valid_temp_mask = (temp_file['temperature_celsius'] > temp_min) & (temp_file['temperature_celsius'] < temp_max)\n",
    "\n",
    "    # Convert the mask to 1 and -1\n",
    "    temp_validity = np.where(valid_temp_mask, 1, -1)\n",
    "\n",
    "    # Use pd.cut to bin df_eda.index based on temp_file['converted_timestamps']\n",
    "    bins = temp_file['converted_timestamps']\n",
    "    df_eda['temp_interval'] = pd.cut(df_eda.index, bins=bins, labels=False, include_lowest=True, right=False)\n",
    "\n",
    "    # Map the interval indices to their corresponding validity values\n",
    "    interval_to_validity = dict(enumerate(temp_validity[:-1]))\n",
    "    df_eda['temp_data_validity'] = df_eda['temp_interval'].map(interval_to_validity)\n",
    "\n",
    "    # Handle values before the first timestamp and after the last timestamp\n",
    "    df_eda.loc[df_eda.index < bins.min(), 'temp_data_validity'] = temp_validity[0]\n",
    "    df_eda.loc[df_eda.index >= bins.max(), 'temp_data_validity'] = temp_validity[-1]\n",
    "\n",
    "    # Drop the temporary 'temp_interval' column\n",
    "    df_eda.drop('temp_interval', axis=1, inplace=True)\n",
    "\n",
    "    #add a new column to df_eda called \"device_recording_validity\" -> -1 if \"device_not_recording\" or \"device_not_worn_correctly\" and +1 if not; for every\n",
    "    # Create a new column in df_eda initialized with NaN\n",
    "    df_eda['device_recording_validity'] = np.nan\n",
    "\n",
    "    # Create a boolean mask for valid recording entries in eda_aggr_file\n",
    "    valid_eda_rec_mask = valid_eda_rec_mask = ~((eda_aggr_file['missing_value_reason'] == 'device_not_recording') | \n",
    "                           (eda_aggr_file['missing_value_reason'] == 'device_not_worn_correctly'))#eda_aggr_file['eda_scl_usiemens'] != np.nan #(eda_aggr_file['missing_value_reason'] != 'device_not_recording') & (eda_aggr_file['missing_value_reason'] != 'device_not_worn_correctly')\n",
    "\n",
    "    # Convert the mask to 1 and -1\n",
    "    dev_rec_validity = np.where(valid_eda_rec_mask, 1, -1)\n",
    "\n",
    "    # Use pd.cut to bin df_eda.index based on eda_aggr_file['converted_timestamps']\n",
    "    bins = eda_aggr_file['converted_timestamps']\n",
    "    df_eda['eda_interval'] = pd.cut(df_eda.index, bins=bins, labels=False, include_lowest=True, right=False)\n",
    "\n",
    "    # Map the interval indices to their corresponding validity values\n",
    "    interval_to_validity = dict(enumerate(dev_rec_validity[:-1]))\n",
    "    df_eda['device_recording_validity'] = df_eda['eda_interval'].map(interval_to_validity)\n",
    "\n",
    "    # Handle values before the first timestamp and after the last timestamp\n",
    "    df_eda.loc[df_eda.index < bins.min(), 'device_recording_validity'] = dev_rec_validity[0]\n",
    "    df_eda.loc[df_eda.index >= bins.max(), 'device_recording_validity'] = dev_rec_validity[-1]\n",
    "\n",
    "    # Drop the temporary 'temp_interval' column\n",
    "    df_eda.drop('eda_interval', axis=1, inplace=True)\n",
    "\n",
    "    \"\"\"    \n",
    "    # Print some diagnostics\n",
    "    print(f\"Number of -1 entries: {(df_eda['device_recording_validity'] == -1).sum()}\")\n",
    "    print(f\"Number of 1 entries: {(df_eda['device_recording_validity'] == 1).sum()}\")\n",
    "    print(f\"Total entries: {len(df_eda)}\")\n",
    "\n",
    "    # Check the distribution of missing_value_reason in eda_aggr_file\n",
    "    print(eda_aggr_file['missing_value_reason'].value_counts())\n",
    "    \"\"\"\n",
    "    #now for all the rows of df_eda, if the \"temp_data_validity\" or \"device_recording_validity\" columns are -1, all eda columns are made NA -> maybe not\n",
    "    return df_eda\n",
    "    \n",
    "###### BVP and HR\n",
    "\n",
    "\n",
    "def bvp_prepro_notag(dir_out, df_bvp, subject, key):\n",
    "    # preprocessing BVP and HR data\n",
    "\n",
    "    # get sampling rate in Hz\n",
    "    sr  = 1/df_bvp['sampRate'].iloc[0]\n",
    "    \n",
    "    # process the data following Elgendi et al. (2013)\n",
    "    # settings: \n",
    "    #   peakwindow=0.111,\n",
    "    #   beatwindow=0.667,\n",
    "    #   beatoffset=0.02,\n",
    "    #   mindelay=0.3\n",
    "    #   minimum peak height of 0\n",
    "    signals, info = nk.ppg_process(df_bvp['bvp'], sampling_rate = sr)\n",
    "    \n",
    "    # plot the data\n",
    "    matplotlib.rcParams['figure.figsize'] = (20, 10)\n",
    "    nk.ppg_plot(signals, info)\n",
    "    plt.savefig(os.path.join(dir_out, subject + '_' + key + '_bvp_signals.png'), dpi = 300)\n",
    "    signals_split = np.array_split(signals, 10)\n",
    "    count = 0\n",
    "    for s in signals_split:\n",
    "        count = count + 1\n",
    "        # graphs can only be created when there are more than three peaks\n",
    "        if sum(s['PPG_Peaks'] == 1) > 3:\n",
    "            nk.ppg_plot(s, info)\n",
    "            plt.savefig(os.path.join(dir_out, subject + '_' + key + '_bvp_signals_' + str(count) + '.png'), dpi = 300)\n",
    "    \n",
    "    # calculate HRV\n",
    "    hrv_indices = nk.hrv(signals, sampling_rate = info['sampling_rate'], show = True)\n",
    "    plt.savefig(os.path.join(dir_out, subject + '_' + key + '_bvp_hrv.png'), dpi = 300)\n",
    "    \n",
    "    # close all figures\n",
    "    plt.close(\"all\") \n",
    "    \n",
    "    # save signals and hrv_indices to csv\n",
    "    signals.index = df_bvp.index\n",
    "    df_bvp = signals.join(df_bvp)\n",
    "    df_bvp.drop(['bvp'], axis=1).rename(columns={\"raw\": \"PPG_Raw_noint\"}).to_csv(os.path.join(dir_out, subject + '_' + key + '_bvp_signals.csv'), index = True)\n",
    "    hrv_indices.to_csv(os.path.join(dir_out, subject + '_' + key + '_bvp_hrv.csv'), index = True)\n",
    "    \n",
    "    return \n",
    "\n",
    "\n",
    "def preproPSYPHY_notag(mainfolder, dir_path, dir_out, subject_list, empatica, date, timezone, exclude = [], winwidth = 8, lowpass = 5, max_art = 50, art_cor = True): #change max_art to 100/3 - the original value\n",
    "\n",
    "    # start writing a log file\n",
    "    log_file_new = os.path.join(mainfolder, 'subject_log.txt')\n",
    "    f = open(log_file_new, \"a\")\n",
    "\n",
    "    # remove excluded participants\n",
    "    \n",
    "    for e in exclude:\n",
    "        for part in subject_list:\n",
    "            if e in part: \n",
    "                subject_list.remove(part)\n",
    "    \n",
    "    \n",
    "    # loop through the participants\n",
    "    for subject in subject_list:\n",
    "        print(subject)\n",
    "       \n",
    "        # print a message\n",
    "        print(simple_colors.blue(datetime.now().strftime(\"%H:%M:%S\") + ' - processing participant ' + subject + ' for date ' + date, 'bold'))\n",
    "        f.write('\\n\\n' + datetime.now().strftime(\"%H:%M:%S\") + ' - processing participant ' + subject + ' for date ' + date)\n",
    "\n",
    "        # read in and convert the data\n",
    "        if empatica == 'e+':\n",
    "\n",
    "            # convert eplus data\n",
    "            dict_data = convert_eplus_notag(dir_path, f, timezone) #RETURN STATMENT 2 - COMMENT OUT THE REST OF THE FUNCTION AND RETURN \n",
    "            #tdf_eda, df_eda, ls_eda = convert_eplus_notag(dir_path, f, timezone) #RETURN STATEMENT 1 - comment out the rest of the this function\n",
    "        \n",
    "        \n",
    "        # if no data was found for this participant, continue with the next one\n",
    "        if len(dict_data) < 1:\n",
    "            continue\n",
    "\n",
    "        print(simple_colors.green(datetime.now().strftime(\"%H:%M:%S\") + ' - conversion done for subject ' + subject + ' for date ' + date, 'bold'))\n",
    "        f.write('\\n' + datetime.now().strftime(\"%H:%M:%S\") + ' - conversion done')\n",
    "\n",
    "        #return dict_data\n",
    "        \n",
    "        \n",
    "        # loop through the blocks and preprocess the data\n",
    "        for key, dict_df in dict_data.items():\n",
    "            print(key)\n",
    "            \n",
    "            # detect artifacts using the EDA Explorer classifier\n",
    "            if key == 'eda':\n",
    "                    #labels  = EDA_artifact_detection(dict_df, dir_out, part, key)\n",
    "                    #data = EDA_artifact_detection(dict_df, dir_out, part, key)\n",
    "                    #data = EDA_artifact_detection(dict_df, dict_data, dir_out, part, key)\n",
    "                    labels = EDA_artifact_detection_notag(dict_data, dir_out, subject, key)\n",
    "                    #print(labels)\n",
    "            elif key == 'temp' and len(dict_df) > 0:# simply save temp and acc, if they exist\n",
    "                try:\n",
    "                    dict_df['temp'].to_csv(os.path.join(dir_out, subject + '_' + key + '_temp.csv'))\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred while saving the 'temp' data: {e}\")\n",
    "                    f.write('\\n' + datetime.now().strftime(\"%H:%M:%S\") + ' - error saving temp data '+ e)\n",
    "                    continue\n",
    "            elif key == 'acc' and len(dict_df) > 0:\n",
    "                try:\n",
    "                    dict_data['acc'].to_csv(os.path.join(dir_out, subject + '_' + key + '_acc.csv'))\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred while saving the 'acc' data: {e}\")\n",
    "                    f.write('\\n' + datetime.now().strftime(\"%H:%M:%S\") + ' - error saving acc data '+ e)\n",
    "                    continue\n",
    "            else:\n",
    "                    continue\n",
    "            #return data\n",
    "            \n",
    "            per_art = sum(labels['Binary'] == -1)*100/len(labels)\n",
    "            print(\"per_art is: \", per_art)\n",
    "            \n",
    "            # add the percent to the tags object            \n",
    "            #tags['artefact%'].loc[(tags['part'] == part) & (tags['tag'] == key)] = per_art\n",
    "            \n",
    "            # only preprocess if less than 20% artefacts\n",
    "            if per_art < max_art:\n",
    "\n",
    "                print(simple_colors.green(datetime.now().strftime(\"%H:%M:%S\") + ' - block ' + key + ': artifact detection done , for date ' + date, 'bold'))\n",
    "                f.write('\\n' + datetime.now().strftime(\"%H:%M:%S\") + ' - block ' + key + ': artifact detection done')\n",
    "\n",
    "                # replacing artefacts with NaNs and then interpolating them\n",
    "                if art_cor:\n",
    "                    #return dict_data['eda'], labels\n",
    "                   \n",
    "                    #df_eda, df_bvp = na_missing(dict_df['eda'], dict_df['bvp'], labels)\n",
    "                    df_eda, df_bvp = na_missing(dict_data['eda'], dict_data['bvp'], labels) #me\n",
    "                    #return df_eda, labels\n",
    "                    \n",
    "                    df_eda, df_bvp, [], [] = int_missing(df_eda, df_bvp, [], [], f)\n",
    "                    #return df_eda\n",
    "                    \n",
    "                    print(simple_colors.green(datetime.now().strftime(\"%H:%M:%S\") + ' - block ' + key + ': artifact correction done, for date ' + date, 'bold'))\n",
    "                    f.write('\\n' + datetime.now().strftime(\"%H:%M:%S\") + ' - block ' + key + ': artifact correction done')\n",
    "                else:\n",
    "                    df_eda = dict_df['eda']\n",
    "                    df_bvp = dict_df['bvp']\n",
    "\n",
    "                # preprocess EDA and BVP data with neurokit\n",
    "                df_eda, df_eda_filtered, signals, info = eda_prepro_notag(dir_out, df_eda, subject, key, winwidth, [], f, timezone) \n",
    "                #return df_eda\n",
    "                \n",
    "                #bvp_prepro_notag(dir_out, df_bvp, subject, key)\n",
    "\n",
    "                \n",
    "                \n",
    "                print(simple_colors.green(datetime.now().strftime(\"%H:%M:%S\") + ' - block ' + key + ': preprocessing done' + ' for date ' + date, 'bold'))\n",
    "                f.write('\\n' + datetime.now().strftime(\"%H:%M:%S\") + ' - block ' + key + ': preprocessing done')\n",
    "\n",
    "                \n",
    "\n",
    "            else: \n",
    "\n",
    "                print(simple_colors.red(datetime.now().strftime(\"%H:%M:%S\") + ' - block ' + key + ': STOPPED due to ' + str(round(per_art,2)) + '% artefacts', 'bold'))\n",
    "                f.write('\\n' + datetime.now().strftime(\"%H:%M:%S\") + ' - block ' + key + ': STOPPED due to ' + str(round(per_art,2)) + '% artefacts')\n",
    "                f.close()\n",
    "                return None, None, None, None\n",
    "                \n",
    "    #tags.to_csv(tag_file[:-4] + '_prepro.csv')\n",
    "    f.close()\n",
    "    return df_eda, df_eda_filtered, signals, info\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f8791c9-e2e9-4d8e-8143-680ee1026ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook l2script_functions.ipynb to script\n",
      "[NbConvertApp] Writing 87161 bytes to l2script_functions.py\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "convert to python script to use in other scripts as needed\n",
    "\"\"\"\n",
    "!jupyter nbconvert --to script l2script_functions.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc2f111-e7dc-412e-a4ea-0cb2d9bb7cb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
