{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2140f0e2-ab4f-40c2-8b54-ddfdc9b6851e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nScript 4: wristband data (mainly: electrodermal activity. Can be modified later on to add other measures if required)\\nReading the raw files from avro to csv\\nExtracting aggr eda, temp and acc (these can be used for plotting or other analyses as required)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "level 1\n",
    "Script 4: wristband data (mainly: electrodermal activity. Can be modified later on to add other measures if required)\n",
    "Reading the raw files from avro to csv\n",
    "Extracting aggr eda, temp and acc (these can be used for plotting or other analyses as required)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "209f2bc6-a714-438b-9193-5406a65d84a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "from avro.datafile import DataFileReader\n",
    "from avro.io import DatumReader\n",
    "\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "061de287-77e1-4150-a2e9-2c641e1c89ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Then functions\n",
    "\"\"\"\n",
    "def utc_cet(utc):\n",
    "    \"\"\"\n",
    "    can modify this function to convert the utc timestamp to whichever timezone needed\n",
    "    \"\"\"\n",
    "    # Specific UTC timestamp in microseconds (Î¼s)\n",
    "    utc_timestamp_microseconds = utc  # Example timestamp\n",
    "\n",
    "    # Convert the microseconds timestamp to seconds since the epoch\n",
    "    utc_timestamp_seconds = utc_timestamp_microseconds / 1_000_000\n",
    "\n",
    "    # Create a datetime object from the timestamp (assumed to be in UTC)\n",
    "    utc_time = datetime.utcfromtimestamp(utc_timestamp_seconds)\n",
    "\n",
    "    # Make the UTC time aware by setting its timezone to UTC\n",
    "    utc_time = utc_time.replace(tzinfo=pytz.utc) \n",
    "\n",
    "    # Define the target timezone (CET)\n",
    "    target_timezone = pytz.timezone('Europe/Berlin')  # Berlin is in the CET zone -> modify this to whichever other timezone required even if different from timezone of device system\n",
    "\n",
    "    # Convert to the desired timezone\n",
    "    cet_time = utc_time.astimezone(target_timezone)\n",
    "\n",
    "    return cet_time\n",
    "\n",
    "def read_avro(avro_file_path, output_dir):\n",
    "    \n",
    "    ## Read Avro file\n",
    "    reader = DataFileReader(open(avro_file_path, \"rb\"), DatumReader())\n",
    "    schema = json.loads(reader.meta.get('avro.schema').decode('utf-8'))\n",
    "    data= next(reader)\n",
    "    ## Print the Avro schema - not a necessary step. remove comments on this code section if step necessary\n",
    "    #print(schema)\n",
    "    #print(\" \")\n",
    "    ## Export sensors data to csv files\n",
    "    \n",
    "    # Accelerometer\n",
    "    acc = data[\"rawData\"][\"accelerometer\"]\n",
    "    timestamp_cet = utc_cet(acc[\"timestampStart\"]) \n",
    "    mod_acc_file = '_'.join([str(timestamp_cet).split(\"+\")[0], 'accelerometer.csv'])\n",
    "    acc_file = mod_acc_file.replace(\":\", \"_\").replace(\".\", \"_\", 1).replace(\" \", \"_\")\n",
    "    timestamp = [round(acc[\"timestampStart\"] + i * (1e6 / acc[\"samplingFrequency\"])) #need to check why this rounding and adding is necessary. For this and other data throughout the function\n",
    "                for i in range(len(acc[\"x\"]))]\n",
    "    timestamp_CET = [utc_cet(timestamp[i])\n",
    "                for i in range(len(acc[\"x\"]))] #for every utc timestamp produced in microseconds it is converted to cet timezone\n",
    "    with open(os.path.join(output_dir, acc_file), 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"unix_timestamp\", \"CET_timestamp\", \"x\", \"y\", \"z\"])\n",
    "        writer.writerows([[ts, tsc, x, y, z] for ts, tsc, x, y, z in zip(timestamp, timestamp_CET, acc[\"x\"], acc[\"y\"], acc[\"z\"])])\n",
    "    \n",
    "    # Gyroscope\n",
    "    gyro = data[\"rawData\"][\"gyroscope\"]\n",
    "    timestamp_cet = utc_cet(gyro[\"timestampStart\"]) \n",
    "    mod_gyro_file = '_'.join([str(timestamp_cet).split(\"+\")[0], 'gyroscope.csv'])\n",
    "    gyro_file = mod_gyro_file.replace(\":\", \"_\").replace(\".\", \"_\", 1).replace(\" \", \"_\") #debug this\n",
    "    timestamp = [round(gyro[\"timestampStart\"] + i * (1e6 / gyro[\"samplingFrequency\"]))\n",
    "                for i in range(len(gyro[\"x\"]))]\n",
    "    timestamp_CET = [utc_cet(timestamp[i])\n",
    "                for i in range(len(gyro[\"x\"]))] #for every utc timestamp produced in microseconds it is converted to cet timezone\n",
    "    with open(os.path.join(output_dir, gyro_file), 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"unix_timestamp\", \"CET_timestamp\", \"x\", \"y\", \"z\"])\n",
    "        writer.writerows([[ts, tsc, x, y, z] for ts, tsc, x, y, z in zip(timestamp, timestamp_CET, gyro[\"x\"], gyro[\"y\"], gyro[\"z\"])])\n",
    "    \n",
    "    # Eda\n",
    "    eda = data[\"rawData\"][\"eda\"]\n",
    "    timestamp_cet = utc_cet(eda[\"timestampStart\"])\n",
    "    mod_eda_file = '_'.join([str(timestamp_cet).split(\"+\")[0], 'eda_cet.csv'])\n",
    "    eda_file = mod_eda_file.replace(\":\", \"_\").replace(\".\", \"_\", 1).replace(\" \", \"_\")\n",
    "    timestamp = [round(eda[\"timestampStart\"] + i * (1e6 / eda[\"samplingFrequency\"])) #(1e6 / eda[\"samplingFrequency\"]) = 10^6 x (1/4) = 10^6 x 0.25s = 250,000 microseconds. Therefore all the subsequent utc timestamps generated should be timestamp start (which is in microseconds) +multiples of 250,000 microseconds and so the answers are all in microseconds\n",
    "                for i in range(len(eda[\"values\"]))]\n",
    "    timestamp_CET = [utc_cet(timestamp[i])\n",
    "                for i in range(len(eda[\"values\"]))] #for every utc timestamp produced in microseconds it is converted to cet timezone\n",
    "    with open(os.path.join(output_dir, eda_file), 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"unix_timestamp\", \"CET_timestamp\", \"eda\"])\n",
    "        writer.writerows([[ts, tsc, eda] for ts, tsc, eda in zip(timestamp, timestamp_CET, eda[\"values\"])])\n",
    "\n",
    "    \n",
    "    # Temperature\n",
    "    tmp = data[\"rawData\"][\"temperature\"]\n",
    "    timestamp_cet = utc_cet(tmp[\"timestampStart\"])\n",
    "    mod_tmp_file = '_'.join([str(timestamp_cet).split(\"+\")[0], 'temperature_cet.csv'])\n",
    "    tmp_file = mod_tmp_file.replace(\":\", \"_\").replace(\".\", \"_\", 1).replace(\" \", \"_\")\n",
    "    timestamp = [round(tmp[\"timestampStart\"] + i * (1e6 / tmp[\"samplingFrequency\"]))\n",
    "                for i in range(len(tmp[\"values\"]))]\n",
    "    timestamp_CET = [utc_cet(timestamp[i])\n",
    "                for i in range(len(tmp[\"values\"]))] #for every utc timestamp produced in microseconds it is converted to cet timezone\n",
    "    with open(os.path.join(output_dir, tmp_file), 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"unix_timestamp\", \"CET_timestamp\", \"temperature\"])\n",
    "        writer.writerows([[ts, tsc, tmp] for ts, tsc, tmp in zip(timestamp, timestamp_CET, tmp[\"values\"])])\n",
    "    \n",
    "    # Tags\n",
    "    tags = data[\"rawData\"][\"tags\"] #need a diff naming strategy. So - using utc timestamp on filename instead\n",
    "    file_timestamp = '_'.join([avro_file_path.split(\"\\\\\")[-1].split(\".\")[0], 'tags.csv']) #will be reusing this for systolic peaks\n",
    "    with open(os.path.join(output_dir, file_timestamp), 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"tags_timestamp\"])\n",
    "        writer.writerows([[tag] for tag in tags[\"tagsTimeMicros\"]])\n",
    "    \n",
    "    # BVP\n",
    "    bvp = data[\"rawData\"][\"bvp\"]\n",
    "    timestamp_cet = utc_cet(bvp[\"timestampStart\"])\n",
    "    mod_bvp_file = '_'.join([str(timestamp_cet).split(\"+\")[0], 'bvp.csv'])\n",
    "    bvp_file = mod_bvp_file.replace(\":\", \"_\").replace(\".\", \"_\", 1).replace(\" \", \"_\")\n",
    "    timestamp = [round(bvp[\"timestampStart\"] + i * (1e6 / bvp[\"samplingFrequency\"]))\n",
    "                for i in range(len(bvp[\"values\"]))]\n",
    "    timestamp_CET = [utc_cet(timestamp[i])\n",
    "                for i in range(len(bvp[\"values\"]))] #for every utc timestamp produced in microseconds it is converted to cet timezone\n",
    "    with open(os.path.join(output_dir, bvp_file), 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"unix_timestamp\", \"CET_timestamp\", \"bvp\"])\n",
    "        writer.writerows([[ts, tsc, bvp] for ts, tsc, bvp in zip(timestamp, timestamp_CET, bvp[\"values\"])])\n",
    "   \n",
    "    # Systolic peaks\n",
    "    sps = data[\"rawData\"][\"systolicPeaks\"] #need a diff naming strategy. So - using utc timestamp on filename instead as in Tags\n",
    "    file_timestamp_sps = '_'.join([avro_file_path.split(\"\\\\\")[-1].split(\".\")[0], 'systolic_peaks.csv'])\n",
    "    with open(os.path.join(output_dir, file_timestamp_sps), 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"systolic_peak_timestamp\"])\n",
    "        writer.writerows([[sp] for sp in sps[\"peaksTimeNanos\"]])\n",
    "    \n",
    "    # Steps\n",
    "    steps = data[\"rawData\"][\"steps\"]\n",
    "    timestamp_cet = utc_cet(steps[\"timestampStart\"])\n",
    "    mod_steps_file = '_'.join([str(timestamp_cet).split(\"+\")[0], 'steps.csv'])\n",
    "    steps_file = mod_steps_file.replace(\":\", \"_\").replace(\".\", \"_\", 1).replace(\" \", \"_\")\n",
    "    timestamp = [round(steps[\"timestampStart\"] + i * (1e6 / steps[\"samplingFrequency\"]))\n",
    "                for i in range(len(steps[\"values\"]))]\n",
    "    timestamp_CET = [utc_cet(timestamp[i])\n",
    "                for i in range(len(steps[\"values\"]))] #for every utc timestamp produced in microseconds it is converted to cet timezone\n",
    "    with open(os.path.join(output_dir, steps_file), 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"unix_timestamp\", \"CET_timestamp\", \"steps\"])\n",
    "        writer.writerows([[ts, tsc, step] for ts, tsc, step in zip(timestamp, timestamp_CET, steps[\"values\"])])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8aa0a2e1-2c53-4eb8-b14a-ce738ba4e16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter the subject folder:  C:\\Users\\Ananya Rao\\Documents\\CAM_LMU\\LMU_Stream_HC_002_4\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Code block to reference the necessary folders\n",
    "\"\"\"\n",
    "parentfolder = input('enter the subject folder: ') \n",
    "\n",
    "\"\"\"\n",
    "names of folders to be used\n",
    "\"\"\"\n",
    "folder1 = 'empatica'\n",
    "folder2 = 'saved_figures'\n",
    "\n",
    "folder11 = 'aggr_p_min'\n",
    "folder12 = 'avro_files'\n",
    "folder13 = 'avro2csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "702e88fd-a90d-4b34-9980-5128b75d2377",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "re-running the essential steps of the code above for files of other days\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "need to generate files for each and every avro file generated for the day. \n",
    "first generate all the new required directories\n",
    "\"\"\"\n",
    "for subfolder in os.listdir(parentfolder):\n",
    "    if subfolder.endswith('_d'):\n",
    "        for file in os.listdir(os.path.join(parentfolder, subfolder, folder1, folder12)):\n",
    "            Dir= os.mkdir(os.path.join(parentfolder, subfolder, folder1, folder13, file.split(\".\")[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5777e5ab-242d-4e55-bde4-518e27dfb30b",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 124\u001b[0m, in \u001b[0;36mread_avro\u001b[1;34m(avro_file_path, output_dir)\u001b[0m\n\u001b[0;32m    123\u001b[0m     writer\u001b[38;5;241m.\u001b[39mwriterow([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystolic_peak_timestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m--> 124\u001b[0m     writer\u001b[38;5;241m.\u001b[39mwriterows([[sp] \u001b[38;5;28;01mfor\u001b[39;00m sp \u001b[38;5;129;01min\u001b[39;00m sps[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpeaksTimeNanos\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# Steps\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 28] No space left on device",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;31mOSError\u001b[0m: [Errno 28] No space left on device",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m ipFile \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(parentfolder, subfolder, folder1, folder12, file_list[i])\n\u001b[0;32m     11\u001b[0m opDir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(parentfolder, subfolder, folder1, folder13, op_list[i])\n\u001b[1;32m---> 12\u001b[0m read_avro(ipFile, opDir)\n",
      "Cell \u001b[1;32mIn[9], line 121\u001b[0m, in \u001b[0;36mread_avro\u001b[1;34m(avro_file_path, output_dir)\u001b[0m\n\u001b[0;32m    119\u001b[0m sps \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrawData\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystolicPeaks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;66;03m#need a diff naming strategy. So - using utc timestamp on filename instead as in Tags\u001b[39;00m\n\u001b[0;32m    120\u001b[0m file_timestamp_sps \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([avro_file_path\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msystolic_peaks.csv\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, file_timestamp_sps), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    122\u001b[0m     writer \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mwriter(f)\n\u001b[0;32m    123\u001b[0m     writer\u001b[38;5;241m.\u001b[39mwriterow([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystolic_peak_timestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "now generate all the new required files to be stored within the newly created directories\n",
    "\"\"\"\n",
    "for subfolder in os.listdir(parentfolder):\n",
    "    if subfolder.endswith('_d'):\n",
    "        \n",
    "        file_list = os.listdir(os.path.join(parentfolder, subfolder, folder1, folder12))\n",
    "        op_list = os.listdir(os.path.join(parentfolder, subfolder, folder1, folder13))\n",
    "        for i in range(0,len(file_list)):\n",
    "            ipFile = os.path.join(parentfolder, subfolder, folder1, folder12, file_list[i])\n",
    "            opDir = os.path.join(parentfolder, subfolder, folder1, folder13, op_list[i])\n",
    "            read_avro(ipFile, opDir) \n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89575c25-cd33-425f-b47c-a978628d5422",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
