{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9257a6b-cda8-46cb-878f-e622a3da0efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Although statistically weak, the mixed effects models from past analysis trials point to some variation of stress levels by period of the day. Divide the stress data from each time bin into morning, afternoon, evening/night for all days. Non-parametric ANOVA\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "1. Import subjective data as before (focus on stress dimension first and then expand to other dimensions)\n",
    "2. Organise them into 15 minute bins\n",
    "3. Group these bins into categories of early morning, morning, noon (afternoon) and night. Use the same time divisions as give_binned_vals_category does to bin values\n",
    "4. Check each group for normality (owing to less number of observations, likely that non-parametric tests needed)\n",
    "5. Conduct non-parametric (or parametric if applicable) ANOVA on the data and tabulate and visualise results\n",
    "6. Do the above for per day and all day (all days together)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6104c4e-82d3-4c21-9b54-03dde193d395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy.stats import shapiro\n",
    "from scipy.stats import kruskal #for independent groups\n",
    "from scipy.stats import friedmanchisquare #for dependent or paired groups\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aa31b6-cfe6-4b7d-ae48-0920323f95d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from l2script_functions import giv_x_y_vals, give_binned_vals, give_binned_vals_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69afb7ec-3548-4112-8d63-a906e7ca6e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder1 = 'empatica'\n",
    "folder2 = 'saved_figures'\n",
    "\n",
    "folder11 = 'aggr_p_min'\n",
    "folder12 = 'avro_files'\n",
    "folder13 = 'avro2csv'\n",
    "folder14 = 'preprocessed_files_debug'\n",
    "folder141 = 'data_preproc_debug'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae29613-16b3-438f-bcc4-8de718ec6b2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#first, need to collect all the TET data for the participant\n",
    "mainfolder = input('enter subject folder: ')\n",
    "ger = True\n",
    "\n",
    "dict_TET_x1, dict_TET_y1,  x_new1, y_new1, x1, y1 = giv_x_y_vals(mainfolder, 'q1', ger) #has duplicates (asd_001)\n",
    "dict_TET_x2, dict_TET_y2,  x_new2, y_new2, x2, y2 = giv_x_y_vals(mainfolder, 'q2', ger) #has duplicates and days with missing data (asd_001)\n",
    "dict_TET_x3, dict_TET_y3,  x_new3, y_new3, x3, y3 = giv_x_y_vals(mainfolder, 'q3', ger) #has duplicates (hc_002) #has days with missing data (asd_001)\n",
    "dict_TET_x4, dict_TET_y4,  x_new4, y_new4, x4, y4 = giv_x_y_vals(mainfolder, 'q4', ger) #has days with missing data (asd_001)\n",
    "dict_TET_x5, dict_TET_y5,  x_new5, y_new5, x5, y5 = giv_x_y_vals(mainfolder, 'q5', ger) #has duplicates and days with missing data (asd_001)\n",
    "dict_TET_x6, dict_TET_y6,  x_new6, y_new6, x6, y6 = giv_x_y_vals(mainfolder, 'q6', ger) #has duplicates (hc_002) #has days with missing data (asd_001)\n",
    "dict_TET_x7, dict_TET_y7,  x_new7, y_new7, x7, y7 = giv_x_y_vals(mainfolder, 'q7', ger)\n",
    "dict_TET_x8, dict_TET_y8,  x_new8, y_new8, x8, y8 = giv_x_y_vals(mainfolder, 'q8', ger) #has duplicates and days with missing data (asd_001) -> but for the day that it had duplicate data (15_3_24_n7_16_3_24_d) the data was identical so all good\n",
    "dict_TET_x9, dict_TET_y9,  x_new9, y_new9, x9, y9 = giv_x_y_vals(mainfolder, 'q9', ger) #has duplicates (hc_002) #has days with missing data (asd_001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203e7287-747e-4a77-9938-ee25d30b977f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, obtain the binned dictionaries for each dimension\n",
    "\"\"\"\n",
    "#e.g:\n",
    "dim_q8 = {}\n",
    "for key in dict_TET_x8:\n",
    "    x_val = (dict_TET_x8[key])*6\n",
    "    y_val = dict_TET_y8[key]\n",
    "    dim_q8[key] = give_binned_vals(x_val, y_val, '15')\n",
    "\"\"\"\n",
    "\n",
    "dict_TET_x = [dict_TET_x1, dict_TET_x2, dict_TET_x3, dict_TET_x4, dict_TET_x5, dict_TET_x6, dict_TET_x7, dict_TET_x8, dict_TET_x9]\n",
    "dict_TET_y = [dict_TET_y1, dict_TET_y2, dict_TET_y3, dict_TET_y4, dict_TET_y5, dict_TET_y6, dict_TET_y7, dict_TET_y8, dict_TET_y9]\n",
    "\n",
    "dim_q = {}\n",
    "\n",
    "for i in range(9):\n",
    "    dim_q[f'dim_q{i+1}'] = {}\n",
    "    for key in dict_TET_x[i]:\n",
    "        x_val = dict_TET_x[i][key] * 6\n",
    "        y_val = dict_TET_y[i][key]\n",
    "        dim_q[f'dim_q{i+1}'][key] = give_binned_vals(x_val, y_val, '15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd2843d-7f96-4bce-aaa3-50a3c94e2618",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for dim in dim_q.keys():\n",
    "    print(dim)\n",
    "    for day in dim_q[dim].keys():\n",
    "        print(day)\n",
    "        \"\"\"\n",
    "for dim in dim_q.keys():   \n",
    "        print(dim)\n",
    "        for day in dim_q[dim].keys():\n",
    "            print(day)\n",
    "            for i in range(0,len(list(dim_q[dim][day].keys()))):\n",
    "                binStartTime = float(list(dim_q[dim][day].keys())[i].split('_')[0])\n",
    "                print(binStartTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5f8a80-4fb8-4061-84e1-e1938f0c6566",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Group these bins into categories of early morning, morning, noon (afternoon) and night. Use the same time divisions as give_binned_vals_category does with bin values\n",
    "#grouped dictionaries for every dimension\n",
    "def group_bin_day_period(dim_q):\n",
    "    bin_arr = np.arange(0,25,6) #going by #3: Group these bins into categories of early morning, morning, noon (afternoon) and night. Use the same time divisions as give_binned_vals_category does with bin values\n",
    "    #grouped dictionaries for every dimension\n",
    "    earlyMorning = {}\n",
    "    morning = {}\n",
    "    afterNoon = {}\n",
    "    night = {}\n",
    "    \n",
    "    for dim in dim_q.keys():\n",
    "        earlyMorning[dim] = {}\n",
    "        morning[dim] = {}\n",
    "        afterNoon[dim] = {}\n",
    "        night[dim] = {}\n",
    "        for day in dim_q[dim].keys():\n",
    "            earlyMorning[dim][day] = []\n",
    "            morning[dim][day] = []\n",
    "            afterNoon[dim][day] = []\n",
    "            night[dim][day] = []\n",
    "            for i in range(0,len(list(dim_q[dim][day].keys()))):\n",
    "                binStartTime = float(list(dim_q[dim][day].keys())[i].split('_')[0])\n",
    "                if binStartTime >= bin_arr[0] and binStartTime < bin_arr[1]:\n",
    "                    earlyMorning[dim][day].append(list(dim_q[dim][day].items())[i])\n",
    "                elif binStartTime >= bin_arr[1] and binStartTime < bin_arr[2]:\n",
    "                    morning[dim][day].append(list(dim_q[dim][day].items())[i])\n",
    "                elif binStartTime >= bin_arr[2] and binStartTime < bin_arr[3]:\n",
    "                    afterNoon[dim][day].append(list(dim_q[dim][day].items())[i])\n",
    "                else:\n",
    "                    night[dim][day].append(list(dim_q[dim][day].items())[i])\n",
    "            earlyMorning[dim][day] = dict(earlyMorning[dim][day])\n",
    "            morning[dim][day] = dict(morning[dim][day])\n",
    "            afterNoon[dim][day] = dict(afterNoon[dim][day])\n",
    "            night[dim][day] = dict(night[dim][day])\n",
    "\n",
    "    return earlyMorning, morning, afterNoon, night\n",
    "                \n",
    "earlyMorning, morning, afterNoon, night = group_bin_day_period(dim_q)\n",
    "night    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9339be4b-b86f-4e43-9d6c-2c83b1039d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "dayPeriod = {'earlyMorning': earlyMorning, 'morning': morning, 'afterNoon': afterNoon, 'night': night}\n",
    "for period, periodDict in dayPeriod.items():\n",
    "    print(period)\n",
    "    #print(periodDict)\n",
    "print(period)\n",
    "#print(periodDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e28b29a-7f5a-4697-b889-52354ef61501",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4 onwards requires a separate set one for per day analysis and the other for all days together\n",
    "\n",
    "#Check each group for normality (owing to less number of observations, likely that non-parametric tests needed)\n",
    "dayPeriod = {'earlyMorning': earlyMorning, 'morning': morning, 'afterNoon': afterNoon, 'night': night}\n",
    "shapiroResults = {}\n",
    "for dim in dim_q:\n",
    "    shapiroResults[dim] = {}\n",
    "    for day in dim_q[dim]:\n",
    "        shapiroResults[dim][day] = {}\n",
    "        for period, periodDict in dayPeriod.items():\n",
    "            shapiroResults[dim][day][period] = {}\n",
    "            #normality test of earlyMorning[dim][day], morning[dim][day], afterNoon[dim][day], night[dim][day]          \n",
    "            filtered_data = {key: value for key, value in periodDict[dim][day].items() if value != -5000}\n",
    "            dataValues = list(filtered_data.values())\n",
    "            if len(dataValues)>2:\n",
    "                    try:\n",
    "                        data_range = max(dataValues) - min(dataValues)\n",
    "                        if data_range == 0:\n",
    "                            print(f\"Warning: Zero range data for {dim}, {day}, {period}\")\n",
    "                            shapiroResults[dim][day][period]['stat'] = None\n",
    "                            shapiroResults[dim][day][period]['p_val'] = None\n",
    "                            shapiroResults[dim][day][period]['normal_yes_or_no'] = None\n",
    "                            shapiroResults[dim][day][period]['data_length'] = len(dataValues)\n",
    "                            continue\n",
    "                        #when range != 0, run shapiro\n",
    "                        stat_eM, p_val_eM = shapiro(dataValues)\n",
    "                        if np.isnan(stat_eM) and not np.isnan(p_val_eM):\n",
    "                            print(f\"Warning: stat is nan but p val not nan for {dim}, {day}, {period}, but p val is {p_val_eM} and length of data after filteration is {len(dataValues)}\")\n",
    "                        if p_val_eM>0.05:\n",
    "                            normal_yn = 1 #normal distribution\n",
    "                        else:\n",
    "                            normal_yn = 0 #not normal distribution\n",
    "                        shapiroResults[dim][day][period]['stat'] = stat_eM\n",
    "                        shapiroResults[dim][day][period]['p_val'] = p_val_eM\n",
    "                        shapiroResults[dim][day][period]['normal_yes_or_no'] = normal_yn\n",
    "                        shapiroResults[dim][day][period]['data_length'] = len(dataValues)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error in Shapiro test for {dim}, {day}, {period}: {str(e)}\")\n",
    "                        shapiroResults[dim][day][period]['stat'] = None\n",
    "                        shapiroResults[dim][day][period]['p_val'] = None\n",
    "                        shapiroResults[dim][day][period]['normal_yes_or_no'] = None\n",
    "                        shapiroResults[dim][day][period]['data_length'] = len(dataValues)\n",
    "            else:\n",
    "                    shapiroResults[dim][day][period]['stat'] = None\n",
    "                    shapiroResults[dim][day][period]['p_val'] = None\n",
    "                    shapiroResults[dim][day][period]['normal_yes_or_no'] = None\n",
    "                    shapiroResults[dim][day][period]['data_length'] = len(dataValues)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56f0fa5-7619-43e1-8cf2-c2640366367d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shapiroResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7059239-c84d-42f4-a8d9-4de564642507",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dayPeriod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479b1af7-c95a-4e8d-8d29-f05b6ea68832",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conduct non-parametric (or parametric if applicable) ANOVA on the data and tabulate and visualise results\n",
    "#KRUSKAL\n",
    "#if a value in any group was missing, the corresponding values in the groups were also taken out. But it is possible that this is not required for a kruskal-wallis test. However, kruskal-wallis test may not be applicable in this use case. So if applicable, evaluate.\n",
    "\n",
    "npAnovaResults = {}\n",
    "\n",
    "for dim in dim_q:\n",
    "    npAnovaResults[dim] = {}\n",
    "    for day in dim_q[dim]:\n",
    "        npAnovaResults[dim][day] = {}\n",
    "        periodDaily = {'earlyMorning': list(earlyMorning[dim][day].values()), 'morning': list(morning[dim][day].values()), 'afterNoon': list(afterNoon[dim][day].values()), 'night': list(night[dim][day].values())}\n",
    "        dfAnovaDay = pd.DataFrame(periodDaily)\n",
    "        \n",
    "        \n",
    "        #if >50%data ==-5000 in a column, drop the column        \n",
    "        for col in dfAnovaDay.columns:\n",
    "            if (dfAnovaDay[col] == -5000).sum() > 0.5 * len(dfAnovaDay):\n",
    "                dfAnovaDay.drop(columns=[col], inplace=True)\n",
    "        \n",
    "       #drop rows where any value in the row is -5000)        \n",
    "        dfAnovaDay = dfAnovaDay[~dfAnovaDay.isin([-5000]).any(axis=1)]\n",
    "        \n",
    "        #each remaining column of the data frame taken as each group\n",
    "       \n",
    "        if len(dfAnovaDay.columns) >= 2:\n",
    "            stat, pVal = kruskal(*[dfAnovaDay[col] for col in dfAnovaDay.columns])        \n",
    "            npAnovaResults[dim][day]['stat'] = stat\n",
    "            npAnovaResults[dim][day]['p_val'] = pVal\n",
    "            npAnovaResults[dim][day]['groups'] = list(dfAnovaDay.columns)\n",
    "            npAnovaResults[dim][day]['final data length'] = len(dfAnovaDay)\n",
    "        else:\n",
    "            npAnovaResults[dim][day]['stat'] = None\n",
    "            npAnovaResults[dim][day]['p_val'] = None\n",
    "            npAnovaResults[dim][day]['groups'] = list(dfAnovaDay.columns)\n",
    "            npAnovaResults[dim][day]['final data length'] = len(dfAnovaDay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5941e847-b896-497a-90de-9752af78308a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "npAnovaResults['dim_q1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c752f1f5-702a-4279-8f8f-554f49dc86fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conduct non-parametric (or parametric if applicable) ANOVA on the data and tabulate and visualise results\n",
    "#FRIEDMAN\n",
    "npAnovaResults = {}\n",
    "for dim in dim_q:\n",
    "    npAnovaResults[dim] = {}\n",
    "    for day in dim_q[dim]:\n",
    "        npAnovaResults[dim][day] = {}\n",
    "        periodDaily = {'earlyMorning': list(earlyMorning[dim][day].values()), 'morning': list(morning[dim][day].values()), 'afterNoon': list(afterNoon[dim][day].values()), 'night': list(night[dim][day].values())}\n",
    "        dfAnovaDay = pd.DataFrame(periodDaily)\n",
    "        \n",
    "        #if >50%data ==-5000 in a column, drop the column   \n",
    "        for col in dfAnovaDay.columns:\n",
    "            if (dfAnovaDay[col] == -5000).sum() > 0.5 * len(dfAnovaDay):\n",
    "                dfAnovaDay.drop(columns=[col], inplace=True)\n",
    "                \n",
    "        #drop rows where any value in the row is -5000)\n",
    "        dfAnovaDay = dfAnovaDay[~dfAnovaDay.isin([-5000]).any(axis=1)]\n",
    "        \n",
    "        #each remaining column of the data frame taken as each group\n",
    "        if len(dfAnovaDay.columns) >= 3:\n",
    "            stat, pVal = friedmanchisquare(*[dfAnovaDay[col] for col in dfAnovaDay.columns])        \n",
    "            npAnovaResults[dim][day]['stat'] = stat\n",
    "            npAnovaResults[dim][day]['p_val'] = pVal\n",
    "            npAnovaResults[dim][day]['groups'] = list(dfAnovaDay.columns)\n",
    "            npAnovaResults[dim][day]['final data length'] = len(dfAnovaDay)\n",
    "        else:\n",
    "            npAnovaResults[dim][day]['stat'] = None\n",
    "            npAnovaResults[dim][day]['p_val'] = None\n",
    "            npAnovaResults[dim][day]['groups'] = list(dfAnovaDay.columns)\n",
    "            npAnovaResults[dim][day]['final data length'] = len(dfAnovaDay)\n",
    "\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239dbcdf-6763-4af2-aef3-f6461a61039a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "npAnovaResults['dim_q8'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baebab9e-b541-421b-844b-74a516b6f3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For all days pooled together\n",
    "#all day npANOVA\n",
    "#friedman\n",
    "\"\"\"\n",
    "\n",
    "npAnovaResultsAllDays = {}\n",
    "\n",
    "for dim in dim_q:  # Iterate over dimensions\n",
    "    # Initialize storage for this dimension\n",
    "    npAnovaResultsAllDays[dim] = {}\n",
    "    \n",
    "    # Aggregate data across all days for each period\n",
    "    periodAllDays = {'earlyMorning': [], 'morning': [], 'afterNoon': [], 'night': []}\n",
    "    for day in dim_q[dim]:\n",
    "        for period in periodAllDays.keys():\n",
    "            # Collect data from all days into the corresponding period\n",
    "            periodAllDays[period].extend(list(eval(period)[dim][day].values()))\n",
    "    \n",
    "    # Create a DataFrame for all days combined\n",
    "    dfAnovaAllDays = pd.DataFrame(periodAllDays)\n",
    "    \n",
    "    # Drop columns if >50% of data is -5000\n",
    "    for col in dfAnovaAllDays.columns:\n",
    "        if (dfAnovaAllDays[col] == -5000).sum() > 0.5 * len(dfAnovaAllDays):\n",
    "            dfAnovaAllDays.drop(columns=[col], inplace=True)\n",
    "    \n",
    "    # Drop rows where any value == -5000\n",
    "    dfAnovaAllDays = dfAnovaAllDays[~dfAnovaAllDays.isin([-5000]).any(axis=1)]\n",
    "    \n",
    "    # Perform friedman test if there are at least 2 groups (columns)\n",
    "    if len(dfAnovaAllDays.columns) >= 3:\n",
    "        stat, pVal = friedmanchisquare(*[dfAnovaAllDays[col] for col in dfAnovaAllDays.columns])\n",
    "        \n",
    "        # Store results\n",
    "        npAnovaResultsAllDays[dim]['stat'] = stat\n",
    "        npAnovaResultsAllDays[dim]['p_val'] = pVal\n",
    "        npAnovaResultsAllDays[dim]['groups'] = list(dfAnovaAllDays.columns)\n",
    "        npAnovaResultsAllDays[dim]['final data length'] = len(dfAnovaAllDays)\n",
    "    else:\n",
    "        # Insufficient data for test\n",
    "        npAnovaResultsAllDays[dim]['stat'] = None\n",
    "        npAnovaResultsAllDays[dim]['p_val'] = None\n",
    "        npAnovaResultsAllDays[dim]['groups'] = list(dfAnovaAllDays.columns)\n",
    "        npAnovaResultsAllDays[dim]['final data length'] = len(dfAnovaAllDays)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afa8d91-c030-4eef-9fa4-dc0349649c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "npAnovaResultsAllDays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c54253-997e-4d98-b67e-541bfdb98556",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOW TO TEST WHICH DAY PERIODS WERE SIGNIFICANTLY DIFFERENT FROM WHICH\n",
    "\n",
    "from scipy.stats import wilcoxon\n",
    "from itertools import combinations\n",
    "try:\n",
    "    from scikit_posthocs import posthoc_nemenyi_friedman\n",
    "except ImportError:\n",
    "    print(\"scikit-posthocs not installed. Only Wilcoxon test will be available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8750390-bfee-4e54-a0e4-f94baea7bfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this before visualisation\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bace3c-f9c8-46b8-92c7-37dd36c3e99b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#all day npANOVA\n",
    "#friedman\n",
    "\n",
    "def friedman_posthoc_with_viz(df, dim_name):\n",
    "    \"\"\"\n",
    "    Perform and visualize post-hoc analysis after significant Friedman test\n",
    "    df: pandas DataFrame where columns are groups\n",
    "    dim_name: name of dimension being analyzed (for plot titles)\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # 1. Wilcoxon with Bonferroni correction\n",
    "    groups = list(df.columns)\n",
    "    n_comparisons = len(groups) * (len(groups) - 1) / 2\n",
    "    alpha = 0.05\n",
    "    bonferroni_alpha = alpha / n_comparisons\n",
    "    \n",
    "    wilcoxon_results = {}\n",
    "    # Matrix to store p-values for heatmap\n",
    "    p_value_matrix = np.zeros((len(groups), len(groups)))\n",
    "    \n",
    "    for i, j in combinations(range(len(groups)), 2):\n",
    "        group1, group2 = groups[i], groups[j]\n",
    "        stat, p_val = wilcoxon(df[group1], df[group2])\n",
    "        wilcoxon_results[f\"{group1} vs {group2}\"] = {\n",
    "            'statistic': stat,\n",
    "            'p_value': p_val,\n",
    "            'significant': p_val < bonferroni_alpha\n",
    "        }\n",
    "        # Fill both sides of the matrix for the heatmap\n",
    "        p_value_matrix[i, j] = p_val\n",
    "        p_value_matrix[j, i] = p_val\n",
    "    \n",
    "    results['wilcoxon'] = wilcoxon_results\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # 1. Box plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.boxplot(data=df)\n",
    "    plt.title(f'Distribution of Values Across Groups\\n{dim_name}')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # 2. Heatmap of p-values\n",
    "    plt.subplot(1, 2, 2)\n",
    "    mask = np.triu(np.ones_like(p_value_matrix, dtype=bool))  # mask upper triangle\n",
    "    sns.heatmap(p_value_matrix, \n",
    "                mask=mask,\n",
    "                xticklabels=groups,\n",
    "                yticklabels=groups,\n",
    "                annot=True,  # Show numbers\n",
    "                fmt='.3f',   # Format to 3 decimal places\n",
    "                cmap='RdYlBu_r',  # Red for significant, blue for non-significant\n",
    "                vmin=0,\n",
    "                vmax=0.05)\n",
    "    plt.title('Pairwise Comparison p-values\\n(significant if < {:.3f})'.format(bonferroni_alpha))\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Print interpretation\n",
    "    print(f\"\\nResults Interpretation for {dim_name}:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Bonferroni-corrected significance level: {bonferroni_alpha:.4f}\")\n",
    "    print(\"\\nSignificant differences found between:\")\n",
    "    significant_pairs = []\n",
    "    for pair, result in wilcoxon_results.items():\n",
    "        if result['significant']:\n",
    "            significant_pairs.append(f\"- {pair} (p={result['p_value']:.4f})\")\n",
    "    if significant_pairs:\n",
    "        print(\"\\n\".join(significant_pairs))\n",
    "    else:\n",
    "        print(\"No significant differences found after Bonferroni correction\")\n",
    "    \n",
    "    return results, plt.gcf()  # Return both results and figure\n",
    "\n",
    "#end of func\n",
    "\n",
    "\n",
    "npAnovaResultsAllDays = {}\n",
    "\n",
    "for dim in dim_q:  # Iterate over dimensions\n",
    "    # Initialize storage for this dimension\n",
    "    npAnovaResultsAllDays[dim] = {}\n",
    "    \n",
    "    # Aggregate data across all days for each period\n",
    "    periodAllDays = {'earlyMorning': [], 'morning': [], 'afterNoon': [], 'night': []}\n",
    "    for day in dim_q[dim]:\n",
    "        for period in periodAllDays.keys():\n",
    "            # Collect data from all days into the corresponding period\n",
    "            periodAllDays[period].extend(list(eval(period)[dim][day].values()))\n",
    "    \n",
    "    #Create a DataFrame for all days combined\n",
    "    dfAnovaAllDays = pd.DataFrame(periodAllDays)\n",
    "\n",
    "     # Drop columns where >50% of data is -5000\n",
    "    for col in dfAnovaAllDays.columns:\n",
    "        if (dfAnovaAllDays[col] == -5000).sum() > 0.5 * len(dfAnovaAllDays):\n",
    "            dfAnovaAllDays.drop(columns=[col], inplace=True)\n",
    "    \n",
    "    # Drop rows where any value is -5000\n",
    "    dfAnovaAllDays = dfAnovaAllDays[~dfAnovaAllDays.isin([-5000]).any(axis=1)]\n",
    "    \n",
    "    # Perform friedmann test if there are at least 2 groups (columns)\n",
    "    if len(dfAnovaAllDays.columns) >= 3 and len(dfAnovaAllDays) > 2 and all(dfAnovaAllDays[col].notna().sum() > 0 for col in dfAnovaAllDays.columns) and len(set(dfAnovaAllDays[col].notna().sum() for col in dfAnovaAllDays.columns)) == 1:\n",
    "        stat, pVal = friedmanchisquare(*[dfAnovaAllDays[col] for col in dfAnovaAllDays.columns])\n",
    "        if pVal < 0.05:  # If Friedman test is significant\n",
    "            posthoc_results, fig  = friedman_posthoc_with_viz(dfAnovaAllDays, dim)\n",
    "            plt.savefig(os.path.join(mainfolder, \"all_day_subjective_dim_corr_post_hoc.png\"), bbox_inches='tight', dpi=300)\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "            npAnovaResultsAllDays[dim]['posthoc'] = posthoc_results\n",
    "        else:\n",
    "            npAnovaResultsAllDays[dim]['posthoc'] = []\n",
    "        # Store results\n",
    "        npAnovaResultsAllDays[dim]['stat'] = stat\n",
    "        npAnovaResultsAllDays[dim]['p_val'] = pVal\n",
    "        npAnovaResultsAllDays[dim]['groups'] = list(dfAnovaAllDays.columns)\n",
    "        npAnovaResultsAllDays[dim]['final data length'] = len(dfAnovaAllDays)\n",
    "    else:\n",
    "        # Insufficient data for test\n",
    "        npAnovaResultsAllDays[dim]['stat'] = None\n",
    "        npAnovaResultsAllDays[dim]['p_val'] = None\n",
    "        npAnovaResultsAllDays[dim]['groups'] = list(dfAnovaAllDays.columns)\n",
    "        npAnovaResultsAllDays[dim]['final data length'] = len(dfAnovaAllDays)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c544a270-54e6-4e55-b27a-50ce9118574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "npAnovaResultsAllDays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b61db8-e7c8-4dfa-9176-14178262605d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAnovaAllDays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635c2bb8-3361-4672-a15b-71444f96e006",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
