{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ba1836-8878-44d7-b2dc-2e523b96c514",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Recreation of l3s8 for objective data derived from digital biomarkers (hereafter referred to as objective data) provided by empatica. This might help see if patterns observed in sbjective data are also seen in the objective data.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "1. Import objective data as before \n",
    "2. Organise them into 15 minute bins\n",
    "3. Group these bins into categories of early morning, morning, noon (afternoon) and night. Use the same time divisions as give_binned_vals_category does to bin values\n",
    "4. Check each group for normality (owing to less number of observations, likely that non-parametric tests needed)\n",
    "5. Conduct non-parametric (or parametric if applicable) ANOVA on the data and tabulate and visualise results\n",
    "6. Do the above for per day and all day (all days together)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4530a7-2e58-47d7-adc4-64244efc804e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy.stats import shapiro\n",
    "from scipy.stats import kruskal #for independent groups\n",
    "from scipy.stats import friedmanchisquare #for dependent or paired groups\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3c758a-ab81-4614-a849-ada81343b02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "define binning function for objective measures\n",
    "\"\"\"\n",
    "def give_binned_vals_obj_meas(df, obj_meas, hour, half_hour, quarter_hour, timezone, category_yn = False):\n",
    "    \n",
    "    bin_dict = {}\n",
    "    bin_dict_scl = {}\n",
    "    bin_dict_scr = {}\n",
    "\n",
    "    #only for objective measures, include the extra hour of data (if in cet)\n",
    "    if category_yn:\n",
    "        bin_arr = np.arange(0,25,6)\n",
    "    elif hour: #hour seperation\n",
    "        bin_arr = np.arange(0,25)\n",
    "    elif half_hour: #half hour seperation\n",
    "        bin_arr = np.arange(0,24.5, 0.5)\n",
    "    elif quarter_hour: #quarter hour seperation\n",
    "        bin_arr = np.arange(0,24.25, 0.25)\n",
    "    # Define the time zones\n",
    "    utc_zone = pytz.utc\n",
    "    req_zone = pytz.timezone(timezone)\n",
    "    #if aggr_p_min data, time conversion block (add an extra column to the dataframe with required timezone timestamps)\n",
    "    def from_isoutc_to_req(iso_timestamp):\n",
    "            # Parse the ISO 8601 timestamp into a datetime object\n",
    "            utc_time = datetime.fromisoformat(iso_timestamp.replace(\"Z\", \"+00:00\"))\n",
    "    \n",
    "            \n",
    "            # Convert to required (cet) time\n",
    "            req_time = utc_time.astimezone(req_zone)\n",
    "            #print(req_time, type(req_time))\n",
    "    \n",
    "            return req_time #.isoformat()\n",
    "\n",
    "    # Apply the conversion function to the 'utc_timestamps' column and create a new column 'converted_timestamps'\n",
    "    df['converted_timestamps'] = df['timestamp_iso'].apply(from_isoutc_to_req)\n",
    "\n",
    "    \n",
    "    first_day = df['converted_timestamps'].iloc[0].day \n",
    "\n",
    "    x_val = df['converted_timestamps'].apply(\n",
    "        lambda x: (24 + int(str(x).split()[1].split(':')[0]) + int(str(x).split()[1].split(':')[1])/60) \n",
    "        if x.day > first_day \n",
    "        else (int(str(x).split()[1].split(':')[0]) + int(str(x).split()[1].split(':')[1])/60)\n",
    "    ).tolist()\n",
    "\n",
    "    \"\"\"\n",
    "    #range of permissible eda range\n",
    "    min_val = 0.05\n",
    "    max_val = 60\n",
    "    not used so far\n",
    "    \"\"\"\n",
    "    if obj_meas == 'eda':\n",
    "        y_val = df['eda_scl_usiemens'].tolist()\n",
    "    elif obj_meas == 'pulse_rate':\n",
    "        y_val = df['pulse_rate_bpm'].tolist()\n",
    "    elif obj_meas == 'prv':\n",
    "        y_val = df['prv_rmssd_ms'].tolist()\n",
    "    elif obj_meas == 'resp_rate':\n",
    "        y_val = df['respiratory_rate_brpm'].tolist()\n",
    "    elif obj_meas == 'temp':\n",
    "        y_val = df['temperature_celsius'].tolist()\n",
    "    elif obj_meas == 'step_count':\n",
    "        y_val = df['step_counts'].tolist()\n",
    "    elif obj_meas == 'acc_std':\n",
    "        y_val = df['accelerometers_std_g'].tolist()\n",
    "    elif obj_meas == 'activity_counts':\n",
    "        y_val = df['activity_counts'].tolist()\n",
    "    elif obj_meas == 'met':\n",
    "        y_val = df['met'].tolist() \n",
    "    elif obj_meas == 'wearing_det':\n",
    "        y_val = df['wearing_detection_percentage'].tolist()\n",
    "    \n",
    "    \n",
    "    for i in range(0, len(bin_arr) - 1):\n",
    "            #Create the key for the dictionary\n",
    "            key = str(bin_arr[i]) + '_' + str(bin_arr[i+1])\n",
    "    \n",
    "            #Initialize an empty list for this key\n",
    "            templst = []\n",
    "            bin_dict[key] = templst\n",
    "        \n",
    "            #Iterate over x_val, append to templst if condition is met\n",
    "            for j in range(0, len(x_val)):\n",
    "                if x_val[j] >= bin_arr[i] and x_val[j] < bin_arr[i+1] and x_val[j]<bin_arr[-1]:\n",
    "                    #Append y_val[j] directly to the list in the dictionary\n",
    "                    bin_dict[key].append(y_val[j])\n",
    "                if x_val[j]>=bin_arr[-1]:\n",
    "                    print(\"not appending value at time: \", x_val[j])\n",
    "\n",
    "    #for conversion of lists to numpy arrays\n",
    "    for key in bin_dict:\n",
    "            bin_dict[key] = np.array(bin_dict[key])\n",
    "            #print(bin_dict)\n",
    "\n",
    "    bin_dict_mean = {}\n",
    "    for key in bin_dict:\n",
    "            if np.all(np.isnan(bin_dict[key])):\n",
    "                #print('list only has nan values')\n",
    "                bin_dict_mean[key] = np.nan\n",
    "            elif ~np.all(np.isnan(bin_dict[key])) and len(bin_dict[key])!=0:\n",
    "                #print('list is not empty')\n",
    "                bin_dict_mean[key] = np.nanmean(bin_dict[key]) \n",
    "            else:\n",
    "                print('-5000 has been appended') #this print statement added to debug if there is ever a situation where this would happen (technically it shouldn't)\n",
    "                bin_dict_mean[key] = -5000\n",
    "\n",
    "    return  df['converted_timestamps'], x_val, y_val, bin_dict_mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d21e58-4c32-471c-a50b-a88506f46e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder1 = 'empatica'\n",
    "folder2 = 'saved_figures'\n",
    "\n",
    "folder11 = 'aggr_p_min'\n",
    "folder12 = 'avro_files'\n",
    "folder13 = 'avro2csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6727d7a0-d3c7-4eb8-ae9d-9ed3f31c996e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Collecting objective data for all\n",
    "\n",
    "mainfolder = input('enter subject folder: ')\n",
    "timezone =  'Europe/Berlin' #default timezone; enter required timezone if different #'utc' this seemed to be used as per the cell: \"Notes, if any\". Maybe revert to this after consideration\n",
    "eda_dict_plot_x = {}\n",
    "eda_dict_plot_y = {}\n",
    "eda_dict_bin = {}\n",
    "pulse_rate_dict_plot_x = {}\n",
    "pulse_rate_dict_plot_y = {}\n",
    "pulse_rate_dict_bin = {}\n",
    "prv_dict_plot_x = {}\n",
    "prv_dict_plot_y = {}\n",
    "prv_dict_bin = {}\n",
    "resp_rate_dict_plot_x = {}\n",
    "resp_rate_dict_plot_y = {}\n",
    "resp_rate_dict_bin = {}\n",
    "temp_dict_plot_x = {}\n",
    "temp_dict_plot_y = {}\n",
    "temp_dict_bin = {}\n",
    "step_dict_plot_x = {}\n",
    "step_dict_plot_y = {}\n",
    "step_dict_bin = {}\n",
    "acc_std_dict_plot_x = {}\n",
    "acc_std_dict_plot_y = {}\n",
    "acc_std_dict_bin = {}\n",
    "activity_dict_plot_x = {}\n",
    "activity_dict_plot_y = {}\n",
    "activity_dict_bin = {}\n",
    "met_dict_plot_x = {}\n",
    "met_dict_plot_y = {}\n",
    "met_dict_bin = {}\n",
    "wearing_det_dict_plot_x = {}\n",
    "wearing_det_dict_plot_y = {}\n",
    "wearing_det_dict_bin = {}\n",
    "\n",
    "\n",
    "\n",
    "#in the lines below, take out \"_converted_timestamp,\" variables after timestamp verification check\n",
    "eda_converted_timestamp = {}\n",
    "pulse_rate_converted_timestamp = {}\n",
    "prv_converted_timestamp = {}\n",
    "resp_rate_converted_timestamp = {}\n",
    "temp_converted_timestamp = {}\n",
    "step_converted_timestamp = {}\n",
    "acc_converted_timestamp = {}\n",
    "activity_converted_timestamp = {}\n",
    "met_converted_timestamp = {}\n",
    "wearing_det_converted_timestamp = {}\n",
    "\n",
    "#storing the dates for which the variables are recorded. Required for time-stitching\n",
    "eda_dates = []\n",
    "pulse_rate_dates = []\n",
    "prv_dates = []\n",
    "resp_rate_dates = []\n",
    "temp_dates = []\n",
    "step_dates = []\n",
    "acc_std_dates = []\n",
    "activity_dates = []\n",
    "met_dates = []\n",
    "wearing_det_dates = []\n",
    "\n",
    "\n",
    "for subfolder in os.listdir(mainfolder):\n",
    "    if subfolder.endswith('d') and os.path.exists(os.path.join(mainfolder, subfolder, folder1, folder11)):\n",
    "        print(subfolder)\n",
    "        for file in os.listdir(os.path.join(mainfolder, subfolder, folder1, folder11)):\n",
    "            if file.endswith('eda.csv'):\n",
    "                eda_df = pd.read_csv(os.path.join(mainfolder, subfolder, folder1, folder11, file))\n",
    "                eda_converted_timestamp[subfolder], eda_dict_plot_x[subfolder], eda_dict_plot_y[subfolder], eda_dict_bin[subfolder] = give_binned_vals_obj_meas(eda_df, 'eda', False, False, True, timezone)\n",
    "                eda_dates.append(subfolder)\n",
    "            \n",
    "            elif file.endswith('pulse-rate.csv'):\n",
    "                pulse_rate_df = pd.read_csv(os.path.join(mainfolder, subfolder, folder1, folder11, file))\n",
    "                pulse_rate_converted_timestamp[subfolder], pulse_rate_dict_plot_x[subfolder], pulse_rate_dict_plot_y[subfolder], pulse_rate_dict_bin[subfolder] = give_binned_vals_obj_meas(pulse_rate_df, 'pulse_rate', False, False, True, timezone)\n",
    "                pulse_rate_dates.append(subfolder)\n",
    "            \n",
    "            elif file.endswith('prv.csv'):\n",
    "                prv_df = pd.read_csv(os.path.join(mainfolder, subfolder, folder1, folder11, file))\n",
    "                prv_converted_timestamp[subfolder], prv_dict_plot_x[subfolder], prv_dict_plot_y[subfolder], prv_dict_bin[subfolder] = give_binned_vals_obj_meas(prv_df, 'prv', False, False, True, timezone)\n",
    "                prv_dates.append(subfolder)\n",
    "                \n",
    "            elif file.endswith('respiratory-rate.csv'):\n",
    "                resp_rate_df = pd.read_csv(os.path.join(mainfolder, subfolder, folder1, folder11, file))\n",
    "                resp_rate_converted_timestamp[subfolder], resp_rate_dict_plot_x[subfolder], resp_rate_dict_plot_y[subfolder], resp_rate_dict_bin[subfolder] = give_binned_vals_obj_meas(resp_rate_df, 'resp_rate', False, False, True, timezone)\n",
    "                resp_rate_dates.append(subfolder)\n",
    "            \n",
    "            elif file.endswith('temperature.csv'):\n",
    "                temp_df = pd.read_csv(os.path.join(mainfolder, subfolder, folder1, folder11, file))\n",
    "                temp_converted_timestamp[subfolder], temp_dict_plot_x[subfolder], temp_dict_plot_y[subfolder], temp_dict_bin[subfolder] = give_binned_vals_obj_meas(temp_df, 'temp', False, False, True, timezone)\n",
    "                temp_dates.append(subfolder)\n",
    "            \n",
    "            elif file.endswith('step-counts.csv'):\n",
    "                step_df = pd.read_csv(os.path.join(mainfolder, subfolder, folder1, folder11, file))\n",
    "                step_converted_timestamp[subfolder], step_dict_plot_x[subfolder], step_dict_plot_y[subfolder], step_dict_bin[subfolder] = give_binned_vals_obj_meas(step_df, 'step_count', False, False, True, timezone)\n",
    "                step_dates.append(subfolder)\n",
    "                \n",
    "            elif file.endswith('accelerometers-std.csv'):\n",
    "                acc_std_df = pd.read_csv(os.path.join(mainfolder, subfolder, folder1, folder11, file))\n",
    "                acc_converted_timestamp[subfolder], acc_std_dict_plot_x[subfolder], acc_std_dict_plot_y[subfolder], acc_std_dict_bin[subfolder] = give_binned_vals_obj_meas(acc_std_df, 'acc_std', False, False, True, timezone)\n",
    "                acc_std_dates.append(subfolder)\n",
    "                \n",
    "            elif file.endswith('activity-counts.csv'):\n",
    "                activity_df = pd.read_csv(os.path.join(mainfolder, subfolder, folder1, folder11, file))\n",
    "                activity_converted_timestamp[subfolder], activity_dict_plot_x[subfolder], activity_dict_plot_y[subfolder], activity_dict_bin[subfolder] = give_binned_vals_obj_meas(activity_df, 'activity_counts', False, False, True, timezone)\n",
    "                activity_dates.append(subfolder)\n",
    "            \n",
    "            elif file.endswith('met.csv'):\n",
    "                met_df = pd.read_csv(os.path.join(mainfolder, subfolder, folder1, folder11, file))\n",
    "                met_converted_timestamp[subfolder], met_dict_plot_x[subfolder], met_dict_plot_y[subfolder], met_dict_bin[subfolder] = give_binned_vals_obj_meas(met_df, 'met', False, False, True, timezone)\n",
    "                met_dates.append(subfolder)\n",
    "            \n",
    "            elif file.endswith('wearing-detection.csv'):\n",
    "                wearing_det_df = pd.read_csv(os.path.join(mainfolder, subfolder, folder1, folder11, file))\n",
    "                wearing_det_converted_timestamp[subfolder], wearing_det_dict_plot_x[subfolder], wearing_det_dict_plot_y[subfolder], wearing_det_dict_bin[subfolder] = give_binned_vals_obj_meas(wearing_det_df, 'wearing_det', False, False, True, timezone)\n",
    "                wearing_det_dates.append(subfolder)                \n",
    "    #(df, obj_meas, hour, half_hour, quarter_hour, timezone):\n",
    "    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf5a0e1-a9e6-47eb-8908-9f603a3008ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc_std_dict_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6075e84-6f94-49cd-9e72-56eecbe55ae4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc_std_dict_bin #after new code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e2453e-a220-48c6-92c8-cecd27b72920",
   "metadata": {},
   "outputs": [],
   "source": [
    "#timestitch\n",
    "#function version of the above\n",
    "def process_time_values(non_cont_dates, dates_list, dict_plot_x, dict_plot_y):\n",
    "    \"\"\"\n",
    "    Process time values to move data points with x >= 24 to the next day.\n",
    "    Also creates binned data and calculates bin means for each date.\n",
    "    \n",
    "    Parameters:\n",
    "    non_cont_dates (list): List of dates that are not continuous i.e; dates where the next night is not the very next date but further off. \n",
    "    dates_list (list): List of dates for the specific measure\n",
    "    dict_plot_x (dict): Dictionary with dates as keys and x-values as values\n",
    "    dict_plot_y (dict): Dictionary with dates as keys and y-values as values\n",
    "    \n",
    "    Returns:\n",
    "    tuple: Modified dict_plot_x, dict_plot_y dictionaries, and bin_dict_mean (nested dictionary)\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    # Create copies to avoid modifying the originals directly\n",
    "    modified_dict_x = {k: v.copy() for k, v in dict_plot_x.items()}\n",
    "    modified_dict_y = {k: v.copy() for k, v in dict_plot_y.items()}\n",
    "    \n",
    "    for i in range(0, len(dates_list)):\n",
    "        current_date = dates_list[i]\n",
    "        \n",
    "        if current_date in non_cont_dates:\n",
    "            # Discard values in dict_plot_x[current_date] that are >= 24 and also corresponding values in dict_plot_y[current_date]\n",
    "            indices_to_remove = [idx for idx, x_val in enumerate(modified_dict_x[current_date]) if x_val >= 24]\n",
    "            \n",
    "            # Remove these values from both x and y arrays (in reverse order to avoid index shifts)\n",
    "            for idx in sorted(indices_to_remove, reverse=True):\n",
    "                modified_dict_x[current_date].pop(idx)\n",
    "                modified_dict_y[current_date].pop(idx)\n",
    "\n",
    "            print(f\"Warning: Discarding values >=24 for the this date {current_date} as it is listed in non_cont_dates.\")\n",
    "        else:\n",
    "            # Check if this isn't the last date\n",
    "            if i + 1 < len(dates_list):\n",
    "                next_date = dates_list[i+1]\n",
    "                print(current_date, next_date)\n",
    "                # Find indices where x values are >= 24\n",
    "                indices_to_move = [idx for idx, x_val in enumerate(modified_dict_x[current_date]) if x_val >= 24]\n",
    "                \n",
    "                if indices_to_move:  # Only process if there are values to move\n",
    "                    # Values to be moved\n",
    "                    x_values_to_move = [modified_dict_x[current_date][idx] - 24 for idx in indices_to_move]  # Subtract 24\n",
    "                    y_values_to_move = [modified_dict_y[current_date][idx] for idx in indices_to_move]\n",
    "                    \n",
    "                    # Add these values to the next day's data\n",
    "                    modified_dict_x[next_date] = x_values_to_move + modified_dict_x[next_date]\n",
    "                    modified_dict_y[next_date] = y_values_to_move + modified_dict_y[next_date]\n",
    "                    \n",
    "                    # Remove these values from the current day (in reverse order to avoid index shifts)\n",
    "                    for idx in sorted(indices_to_move, reverse=True):\n",
    "                        modified_dict_x[current_date].pop(idx)\n",
    "                        modified_dict_y[current_date].pop(idx)\n",
    "            else:\n",
    "                # This is the last date, so we can't move values to the next day\n",
    "                print(f\"Warning: Discarding values >=24 for the last date {current_date} as there's no next day.\")\n",
    "                indices_to_remove = [idx for idx, x_val in enumerate(modified_dict_x[current_date]) if x_val >= 24]\n",
    "                \n",
    "                # Remove these values (in reverse order to avoid index shifts)\n",
    "                for idx in sorted(indices_to_remove, reverse=True):\n",
    "                    modified_dict_x[current_date].pop(idx)\n",
    "                    modified_dict_y[current_date].pop(idx)\n",
    "    \n",
    "    # Create binned data for each date\n",
    "    bin_dict_mean = {}\n",
    "    \n",
    "    # Process each date separately\n",
    "    for date in dates_list:\n",
    "        x_val = modified_dict_x[date]\n",
    "        y_val = modified_dict_y[date]\n",
    "        \n",
    "        # Create bin dictionary for this date\n",
    "        bin_dict = {}\n",
    "        \n",
    "        # Create bins\n",
    "        bin_arr = np.arange(0, 24.25, 0.25)\n",
    "        \n",
    "        for i in range(0, len(bin_arr) - 1):\n",
    "            # Create the key for the dictionary\n",
    "            key = str(bin_arr[i]) + '_' + str(bin_arr[i+1])\n",
    "            \n",
    "            # Initialize an empty list for this key\n",
    "            templst = []\n",
    "            bin_dict[key] = templst\n",
    "            \n",
    "            # Iterate over x_val, append to templst if condition is met\n",
    "            for j in range(0, len(x_val)):\n",
    "                if x_val[j] >= bin_arr[i] and x_val[j] < bin_arr[i+1] and x_val[j] < bin_arr[-1]:\n",
    "                    # Append y_val[j] directly to the list in the dictionary\n",
    "                    bin_dict[key].append(y_val[j])\n",
    "                if x_val[j] >= bin_arr[-1]:\n",
    "                    print(f\"Date {date}: not appending value at time: {x_val[j]}\")\n",
    "        \n",
    "        # Convert lists to numpy arrays\n",
    "        for key in bin_dict:\n",
    "            bin_dict[key] = np.array(bin_dict[key])\n",
    "        \n",
    "        # Calculate means for this date's bins\n",
    "        date_bin_dict_mean = {}\n",
    "        for key in bin_dict:\n",
    "            if len(bin_dict[key]) == 0:\n",
    "                date_bin_dict_mean[key] = np.nan\n",
    "            elif np.all(np.isnan(bin_dict[key])):\n",
    "                # List only has nan values\n",
    "                date_bin_dict_mean[key] = np.nan\n",
    "            elif ~np.all(np.isnan(bin_dict[key])) and len(bin_dict[key]) != 0:\n",
    "                # List is not empty and contains non-nan values\n",
    "                date_bin_dict_mean[key] = np.nanmean(bin_dict[key]) \n",
    "            else:\n",
    "                print(f'Date {date}: -5000 has been appended')  # Debug statement\n",
    "                date_bin_dict_mean[key] = -5000\n",
    "        \n",
    "        # Add this date's bin means to the overall dictionary\n",
    "        bin_dict_mean[date] = date_bin_dict_mean\n",
    "    \n",
    "    return modified_dict_x, modified_dict_y, bin_dict_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8200fa1c-3d27-4be8-8e85-f4e951967c00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Process each measure\n",
    "non_cont_dates = [\"09_3_24_n2_10_3_24_d\"] #for asd_001\n",
    "eda_dict_plot_x, eda_dict_plot_y, eda_dict_bin = process_time_values(non_cont_dates, eda_dates, eda_dict_plot_x, eda_dict_plot_y)\n",
    "pulse_rate_dict_plot_x, pulse_rate_dict_plot_y, pulse_rate_dict_bin = process_time_values(non_cont_dates, pulse_rate_dates, pulse_rate_dict_plot_x, pulse_rate_dict_plot_y)\n",
    "prv_dict_plot_x, prv_dict_plot_y, prv_dict_bin = process_time_values(non_cont_dates, prv_dates, prv_dict_plot_x, prv_dict_plot_y)\n",
    "resp_rate_dict_plot_x, resp_rate_dict_plot_y, resp_rate_dict_bin = process_time_values(non_cont_dates, resp_rate_dates, resp_rate_dict_plot_x, resp_rate_dict_plot_y)\n",
    "temp_dict_plot_x, temp_dict_plot_y, temp_dict_bin = process_time_values(non_cont_dates, temp_dates, temp_dict_plot_x, temp_dict_plot_y)\n",
    "step_dict_plot_x, step_dict_plot_y, step_dict_bin = process_time_values(non_cont_dates, step_dates, step_dict_plot_x, step_dict_plot_y)\n",
    "acc_std_dict_plot_x, acc_std_dict_plot_y, acc_std_dict_bin = process_time_values(non_cont_dates, acc_std_dates, acc_std_dict_plot_x, acc_std_dict_plot_y)\n",
    "activity_dict_plot_x, activity_dict_plot_y, activity_dict_bin = process_time_values(non_cont_dates, activity_dates, activity_dict_plot_x, activity_dict_plot_y)\n",
    "met_dict_plot_x, met_dict_plot_y, met_dict_bin = process_time_values(non_cont_dates, met_dates, met_dict_plot_x, met_dict_plot_y)\n",
    "wearing_det_dict_plot_x, wearing_det_dict_plot_y, wearing_det_dict_bin = process_time_values(non_cont_dates, wearing_det_dates, wearing_det_dict_plot_x, wearing_det_dict_plot_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a1eb9f-ac65-4c09-968b-963d1e07e356",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to collect each binned objective measure into separate dictionaries each containing one measure for all days \n",
    "\n",
    "list_meas = [eda_dict_bin, pulse_rate_dict_bin, prv_dict_bin, resp_rate_dict_bin, temp_dict_bin, step_dict_bin, acc_std_dict_bin, activity_dict_bin, met_dict_bin, wearing_det_dict_bin]\n",
    "\n",
    "meas = {}\n",
    "\n",
    "list_name_meas = ['eda_dict_bin', 'pulse_rate_dict_bin', 'prv_dict_bin', 'resp_rate_dict_bin', \n",
    "                  'temp_dict_bin', 'step_dict_bin', 'acc_std_dict_bin', 'activity_dict_bin', \n",
    "                  'met_dict_bin', 'wearing_det_dict_bin']\n",
    "\n",
    "dict_meas = {}\n",
    "\n",
    "for item_name, item in zip(list_name_meas, list_meas):\n",
    "    dict_meas[item_name] = item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc59eb2-dd69-49de-8675-3d4a34a0a83f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dict_meas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaa3669-b82c-49b9-878e-dd85614f036f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#From this cell onwards, borrowed from l3s8\n",
    "#Group these bins into categories of early morning, morning, noon (afternoon) and night. Use the same time divisions as give_binned_vals_category does to bin values\n",
    "#grouped dictionaries for every dimension\n",
    "def group_bin_day_period(dim_q):\n",
    "    bin_arr = np.arange(0,25,6) #going by #3: Group these bins into categories of early morning, morning, noon (afternoon) and night. Use the same time divisions as give_binned_vals_category does to bin values\n",
    "    #grouped dictionaries for every dimension\n",
    "    earlyMorning = {}\n",
    "    morning = {}\n",
    "    afterNoon = {}\n",
    "    night = {}\n",
    "    \n",
    "    for dim in dim_q.keys():\n",
    "        earlyMorning[dim] = {}\n",
    "        morning[dim] = {}\n",
    "        afterNoon[dim] = {}\n",
    "        night[dim] = {}\n",
    "        for day in dim_q[dim].keys():\n",
    "            earlyMorning[dim][day] = []\n",
    "            morning[dim][day] = []\n",
    "            afterNoon[dim][day] = []\n",
    "            night[dim][day] = []\n",
    "            for i in range(0,len(list(dim_q[dim][day].keys()))):\n",
    "                binStartTime = float(list(dim_q[dim][day].keys())[i].split('_')[0])\n",
    "                if binStartTime >= bin_arr[0] and binStartTime < bin_arr[1]:\n",
    "                    earlyMorning[dim][day].append(list(dim_q[dim][day].items())[i])\n",
    "                elif binStartTime >= bin_arr[1] and binStartTime < bin_arr[2]:\n",
    "                    morning[dim][day].append(list(dim_q[dim][day].items())[i])\n",
    "                elif binStartTime >= bin_arr[2] and binStartTime < bin_arr[3]:\n",
    "                    afterNoon[dim][day].append(list(dim_q[dim][day].items())[i])\n",
    "                else:\n",
    "                    night[dim][day].append(list(dim_q[dim][day].items())[i])\n",
    "            earlyMorning[dim][day] = dict(earlyMorning[dim][day])\n",
    "            morning[dim][day] = dict(morning[dim][day])\n",
    "            afterNoon[dim][day] = dict(afterNoon[dim][day])\n",
    "            night[dim][day] = dict(night[dim][day])\n",
    "\n",
    "    return earlyMorning, morning, afterNoon, night\n",
    "                \n",
    "earlyMorning, morning, afterNoon, night = group_bin_day_period(dict_meas)\n",
    "night    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cb02f8-6a58-4fbe-91b6-bcf6a8223254",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "earlyMorning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906e1555-cf56-4dc8-826f-fc6573e07200",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Step 4 onwards requires a separate set one for per day analysis and the other for all days together \n",
    "\n",
    "#Check each group for normality (owing to less number of observations, likely that non-parametric tests needed)\n",
    "dayPeriod = {'earlyMorning': earlyMorning, 'morning': morning, 'afterNoon': afterNoon, 'night': night}\n",
    "shapiroResults = {}\n",
    "for dim in dict_meas:\n",
    "    shapiroResults[dim] = {}\n",
    "    for day in dict_meas[dim]:\n",
    "        shapiroResults[dim][day] = {}\n",
    "        for period, periodDict in dayPeriod.items():\n",
    "            shapiroResults[dim][day][period] = {}\n",
    "            #normality test of earlyMorning[dim][day], morning[dim][day], afterNoon[dim][day], night[dim][day]          \n",
    "            filtered_data = {key: value for key, value in periodDict[dim][day].items() if not np.isnan(value)}\n",
    "            dataValues = list(filtered_data.values())\n",
    "            if len(dataValues)>2:\n",
    "                    try:\n",
    "                        data_range = max(dataValues) - min(dataValues)\n",
    "                        if data_range == 0:\n",
    "                            print(f\"Warning: Zero range data for {dim}, {day}, {period}\")\n",
    "                            shapiroResults[dim][day][period]['stat'] = None\n",
    "                            shapiroResults[dim][day][period]['p_val'] = None\n",
    "                            shapiroResults[dim][day][period]['normal_yes_or_no'] = None\n",
    "                            shapiroResults[dim][day][period]['data_length'] = len(dataValues)\n",
    "                            continue\n",
    "                        #when range != 0, run shapiro\n",
    "                        stat_eM, p_val_eM = shapiro(dataValues)\n",
    "                        if np.isnan(stat_eM) and not np.isnan(p_val_eM):\n",
    "                            print(f\"Warning: stat is nan but p val not nan for {dim}, {day}, {period}, but p val is {p_val_eM} and length of data after filteration is {len(dataValues)}\")\n",
    "                        if p_val_eM>0.05:\n",
    "                            normal_yn = 1 #normal distribution\n",
    "                        else:\n",
    "                            normal_yn = 0 #not normal distribution\n",
    "                        shapiroResults[dim][day][period]['stat'] = stat_eM\n",
    "                        shapiroResults[dim][day][period]['p_val'] = p_val_eM\n",
    "                        shapiroResults[dim][day][period]['normal_yes_or_no'] = normal_yn\n",
    "                        shapiroResults[dim][day][period]['data_length'] = len(dataValues)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error in Shapiro test for {dim}, {day}, {period}: {str(e)}\")\n",
    "                        shapiroResults[dim][day][period]['stat'] = None\n",
    "                        shapiroResults[dim][day][period]['p_val'] = None\n",
    "                        shapiroResults[dim][day][period]['normal_yes_or_no'] = None\n",
    "                        shapiroResults[dim][day][period]['data_length'] = len(dataValues)\n",
    "            else:\n",
    "                    shapiroResults[dim][day][period]['stat'] = None\n",
    "                    shapiroResults[dim][day][period]['p_val'] = None\n",
    "                    shapiroResults[dim][day][period]['normal_yes_or_no'] = None\n",
    "                    shapiroResults[dim][day][period]['data_length'] = len(dataValues)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c27557-f668-432e-b1d6-5cbdcb42ed94",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716e88b8-755b-406d-9c86-55769e2f2578",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab1b391-208d-47c5-b0d9-d6b8572639f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shapiroResults['wearing_det_dict_bin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6f4ee9-aaf7-4f62-a3ee-2b8a3abb5571",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shapiroResults['eda_dict_bin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef623fc5-6fcc-45fa-8d14-27405ad7b440",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dayPeriod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215a7e72-ad7d-4cca-b1f1-b5c2929a1d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conduct non-parametric (or parametric if applicable) ANOVA on the data and tabulate and visualise results\n",
    "#FRIEDMAN\n",
    "npAnovaResults = {}\n",
    "for dim in dict_meas:\n",
    "    npAnovaResults[dim] = {}\n",
    "    for day in dict_meas[dim]:\n",
    "        npAnovaResults[dim][day] = {}\n",
    "        periodDaily = {'earlyMorning': list(earlyMorning[dim][day].values()), 'morning': list(morning[dim][day].values()), 'afterNoon': list(afterNoon[dim][day].values()), 'night': list(night[dim][day].values())}\n",
    "        #periodDaily = {'earlyMorning': earlyMorning[dim][day], 'morning': morning[dim][day], 'afterNoon': afterNoon[dim][day], 'night': night[dim][day]}\n",
    "        dfAnovaDay = pd.DataFrame(periodDaily)\n",
    "        \"\"\"\n",
    "        for col in list(dfAnovaDay.columns()):\n",
    "            #if >50%data ==np.nan in that column, drop the column\n",
    "        \"\"\"\n",
    "        for col in dfAnovaDay.columns:\n",
    "            if (dfAnovaDay[col] == np.nan).sum() > 0.5 * len(dfAnovaDay):\n",
    "                dfAnovaDay.drop(columns=[col], inplace=True)\n",
    "        \"\"\"\n",
    "        df.drop(#rows where any value in the row is np.nan)\n",
    "        \"\"\"\n",
    "        dfAnovaDay = dfAnovaDay[~dfAnovaDay.isin([np.nan]).any(axis=1)]\n",
    "        \"\"\"\n",
    "        stat, pVal = friedman(#each remaining column of the data frame as each group)\n",
    "        \"\"\"\n",
    "        dfAnovaDay = dfAnovaDay.apply(pd.to_numeric, errors='coerce')\n",
    "        dfAnovaDay = dfAnovaDay.dropna()  # drop any new NaNs from conversion\n",
    "        if len(dfAnovaDay.columns) >= 3 and len(dfAnovaDay) > 2 and all(dfAnovaDay[col].notna().sum() > 0 for col in dfAnovaDay.columns) and len(set(dfAnovaDay[col].notna().sum() for col in dfAnovaDay.columns)) == 1:\n",
    "            stat, pVal = friedmanchisquare(*[dfAnovaDay[col] for col in dfAnovaDay.columns])  #DEBUG: ERROR BEING GENERATED HERE FOR ASD_001 POSSIBLY FOR EDA VARIABLE. LOOKS LIKE SOME VARIABLE K IS = 1. BUT WHAT IS K AND WHERE IS IT COMING FROM?      \n",
    "            npAnovaResults[dim][day]['stat'] = stat\n",
    "            npAnovaResults[dim][day]['p_val'] = pVal\n",
    "            npAnovaResults[dim][day]['groups'] = list(dfAnovaDay.columns)\n",
    "            npAnovaResults[dim][day]['final data length'] = len(dfAnovaDay)\n",
    "        else:\n",
    "            npAnovaResults[dim][day]['stat'] = None\n",
    "            npAnovaResults[dim][day]['p_val'] = None\n",
    "            npAnovaResults[dim][day]['groups'] = list(dfAnovaDay.columns)\n",
    "            npAnovaResults[dim][day]['final data length'] = len(dfAnovaDay)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0385a592-5800-450b-8689-dbb0473dbde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAnovaDay\n",
    "dim\n",
    "day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c1a18d-630c-4934-9256-138215f72cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "from itertools import combinations\n",
    "try:\n",
    "    from scikit_posthocs import posthoc_nemenyi_friedman\n",
    "except ImportError:\n",
    "    print(\"scikit-posthocs not installed. Only Wilcoxon test will be available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61937ba-3b26-4e73-baae-62a33d1f7e9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#all day npANOVA\n",
    "#friedman\n",
    "\n",
    "def friedman_posthoc_with_viz(df, dim_name):\n",
    "    \"\"\"\n",
    "    Perform and visualize post-hoc analysis after significant Friedman test\n",
    "    df: pandas DataFrame where columns are groups\n",
    "    dim_name: name of dimension being analyzed (for plot titles)\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # 1. Wilcoxon with Bonferroni correction\n",
    "    groups = list(df.columns)\n",
    "    n_comparisons = len(groups) * (len(groups) - 1) / 2\n",
    "    alpha = 0.05\n",
    "    bonferroni_alpha = alpha / n_comparisons\n",
    "    \n",
    "    wilcoxon_results = {}\n",
    "    # Matrix to store p-values for heatmap\n",
    "    p_value_matrix = np.zeros((len(groups), len(groups)))\n",
    "    \n",
    "    for i, j in combinations(range(len(groups)), 2):\n",
    "        group1, group2 = groups[i], groups[j]\n",
    "        stat, p_val = wilcoxon(df[group1], df[group2])\n",
    "        wilcoxon_results[f\"{group1} vs {group2}\"] = {\n",
    "            'statistic': stat,\n",
    "            'p_value': p_val,\n",
    "            'significant': p_val < bonferroni_alpha\n",
    "        }\n",
    "        # Fill both sides of the matrix for the heatmap\n",
    "        p_value_matrix[i, j] = p_val\n",
    "        p_value_matrix[j, i] = p_val\n",
    "    \n",
    "    results['wilcoxon'] = wilcoxon_results\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # 1. Box plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.boxplot(data=df)\n",
    "    plt.title(f'Distribution of Values Across Groups\\n{dim_name}')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # 2. Heatmap of p-values\n",
    "    plt.subplot(1, 2, 2)\n",
    "    mask = np.triu(np.ones_like(p_value_matrix, dtype=bool))  # mask upper triangle\n",
    "    sns.heatmap(p_value_matrix, \n",
    "                mask=mask,\n",
    "                xticklabels=groups,\n",
    "                yticklabels=groups,\n",
    "                annot=True,  # Show numbers\n",
    "                fmt='.3f',   # Format to 3 decimal places\n",
    "                cmap='RdYlBu_r',  # Red for significant, blue for non-significant\n",
    "                vmin=0,\n",
    "                vmax=0.05)\n",
    "    plt.title('Pairwise Comparison p-values\\n(significant if < {:.3f})'.format(bonferroni_alpha))\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Print interpretation\n",
    "    print(f\"\\nResults Interpretation for {dim_name}:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Bonferroni-corrected significance level: {bonferroni_alpha:.4f}\")\n",
    "    print(\"\\nSignificant differences found between:\")\n",
    "    significant_pairs = []\n",
    "    for pair, result in wilcoxon_results.items():\n",
    "        if result['significant']:\n",
    "            significant_pairs.append(f\"- {pair} (p={result['p_value']:.4f})\")\n",
    "    if significant_pairs:\n",
    "        print(\"\\n\".join(significant_pairs))\n",
    "    else:\n",
    "        print(\"No significant differences found after Bonferroni correction\")\n",
    "    \n",
    "    return results, plt.gcf()  # Return both results and figure\n",
    "\n",
    "#end of func\n",
    "\n",
    "\n",
    "npAnovaResultsAllDays = {}\n",
    "\n",
    "for dim in dict_meas:  # Iterate over dimensions\n",
    "    # Initialize storage for this dimension\n",
    "    npAnovaResultsAllDays[dim] = {}\n",
    "    \n",
    "    # Aggregate data across all days for each period\n",
    "    periodAllDays = {'earlyMorning': [], 'morning': [], 'afterNoon': [], 'night': []}\n",
    "    for day in dict_meas[dim]:\n",
    "        for period in periodAllDays.keys():\n",
    "            # Collect data from all days into the corresponding period\n",
    "            periodAllDays[period].extend(list(eval(period)[dim][day].values()))\n",
    "    \n",
    "    #Create a DataFrame for all days combined\n",
    "    dfAnovaAllDays = pd.DataFrame(periodAllDays)\n",
    "    \n",
    "    #dropping columns where >50% of data is np.nan\n",
    "    for col in dfAnovaAllDays.columns:\n",
    "        if (dfAnovaAllDays[col] == np.nan).sum() > 0.5 * len(dfAnovaAllDays):\n",
    "            dfAnovaAllDays.drop(columns=[col], inplace=True)\n",
    "    \n",
    "    # Drop rows where any value is np.nan\n",
    "    dfAnovaAllDays = dfAnovaAllDays[~dfAnovaAllDays.isin([np.nan]).any(axis=1)]\n",
    "    \n",
    "    # Perform friedmann test if there are at least 2 groups (columns)\n",
    "    if len(dfAnovaAllDays.columns) >= 3:\n",
    "        stat, pVal = friedmanchisquare(*[dfAnovaAllDays[col] for col in dfAnovaAllDays.columns])\n",
    "        if pVal < 0.05:  # If Friedman test is significant\n",
    "            posthoc_results, fig  = friedman_posthoc_with_viz(dfAnovaAllDays, dim)\n",
    "            plt.savefig(os.path.join(mainfolder, f\"{dim}_all_day_subjective_dim_corr_post_hoc.png\"), bbox_inches='tight', dpi=300)\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "            npAnovaResultsAllDays[dim]['posthoc'] = posthoc_results\n",
    "        else:\n",
    "            npAnovaResultsAllDays[dim]['posthoc'] = []\n",
    "        # Store results\n",
    "        npAnovaResultsAllDays[dim]['stat'] = stat\n",
    "        npAnovaResultsAllDays[dim]['p_val'] = pVal\n",
    "        npAnovaResultsAllDays[dim]['groups'] = list(dfAnovaAllDays.columns)\n",
    "        npAnovaResultsAllDays[dim]['final data length'] = len(dfAnovaAllDays)\n",
    "    else:\n",
    "        # Insufficient data for test\n",
    "        npAnovaResultsAllDays[dim]['stat'] = None\n",
    "        npAnovaResultsAllDays[dim]['p_val'] = None\n",
    "        npAnovaResultsAllDays[dim]['groups'] = list(dfAnovaAllDays.columns)\n",
    "        npAnovaResultsAllDays[dim]['final data length'] = len(dfAnovaAllDays)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15983058-fffe-4a00-86a7-f179e629d974",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#after timestitch\n",
    "#all day npANOVA\n",
    "#friedman\n",
    "\n",
    "def friedman_posthoc_with_viz(df, dim_name):\n",
    "    \"\"\"\n",
    "    Perform and visualize post-hoc analysis after significant Friedman test\n",
    "    df: pandas DataFrame where columns are groups\n",
    "    dim_name: name of dimension being analyzed (for plot titles)\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # 1. Wilcoxon with Bonferroni correction\n",
    "    groups = list(df.columns)\n",
    "    n_comparisons = len(groups) * (len(groups) - 1) / 2\n",
    "    alpha = 0.05\n",
    "    bonferroni_alpha = alpha / n_comparisons\n",
    "    \n",
    "    wilcoxon_results = {}\n",
    "    # Matrix to store p-values for heatmap\n",
    "    p_value_matrix = np.zeros((len(groups), len(groups)))\n",
    "    \n",
    "    for i, j in combinations(range(len(groups)), 2):\n",
    "        group1, group2 = groups[i], groups[j]\n",
    "        stat, p_val = wilcoxon(df[group1], df[group2])\n",
    "        wilcoxon_results[f\"{group1} vs {group2}\"] = {\n",
    "            'statistic': stat,\n",
    "            'p_value': p_val,\n",
    "            'significant': p_val < bonferroni_alpha\n",
    "        }\n",
    "        # Fill both sides of the matrix for the heatmap\n",
    "        p_value_matrix[i, j] = p_val\n",
    "        p_value_matrix[j, i] = p_val\n",
    "    \n",
    "    results['wilcoxon'] = wilcoxon_results\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # 1. Box plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.boxplot(data=df)\n",
    "    plt.title(f'Distribution of Values Across Groups\\n{dim_name}')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # 2. Heatmap of p-values\n",
    "    plt.subplot(1, 2, 2)\n",
    "    mask = np.triu(np.ones_like(p_value_matrix, dtype=bool))  # mask upper triangle\n",
    "    sns.heatmap(p_value_matrix, \n",
    "                mask=mask,\n",
    "                xticklabels=groups,\n",
    "                yticklabels=groups,\n",
    "                annot=True,  # Show numbers\n",
    "                fmt='.3f',   # Format to 3 decimal places\n",
    "                cmap='RdYlBu_r',  # Red for significant, blue for non-significant\n",
    "                vmin=0,\n",
    "                vmax=0.05)\n",
    "    plt.title('Pairwise Comparison p-values\\n(significant if < {:.3f})'.format(bonferroni_alpha))\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Print interpretation\n",
    "    print(f\"\\nResults Interpretation for {dim_name}:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Bonferroni-corrected significance level: {bonferroni_alpha:.4f}\")\n",
    "    print(\"\\nSignificant differences found between:\")\n",
    "    significant_pairs = []\n",
    "    for pair, result in wilcoxon_results.items():\n",
    "        if result['significant']:\n",
    "            significant_pairs.append(f\"- {pair} (p={result['p_value']:.4f})\")\n",
    "    if significant_pairs:\n",
    "        print(\"\\n\".join(significant_pairs))\n",
    "    else:\n",
    "        print(\"No significant differences found after Bonferroni correction\")\n",
    "    \n",
    "    return results, plt.gcf()  # Return both results and figure\n",
    "\n",
    "#end of func\n",
    "\n",
    "\n",
    "npAnovaResultsAllDays = {}\n",
    "\n",
    "for dim in dict_meas:  # Iterate over dimensions\n",
    "    # Initialize storage for this dimension\n",
    "    npAnovaResultsAllDays[dim] = {}\n",
    "    \n",
    "    # Aggregate data across all days for each period\n",
    "    periodAllDays = {'earlyMorning': [], 'morning': [], 'afterNoon': [], 'night': []}\n",
    "    for day in dict_meas[dim]:\n",
    "        for period in periodAllDays.keys():\n",
    "            # Collect data from all days into the corresponding period\n",
    "            periodAllDays[period].extend(list(eval(period)[dim][day].values()))\n",
    "    \n",
    "    #Create a DataFrame for all days combined\n",
    "    dfAnovaAllDays = pd.DataFrame(periodAllDays)\n",
    "    \n",
    "    #dropping columns where >50% of data is np.nan\n",
    "    for col in dfAnovaAllDays.columns:\n",
    "        if (dfAnovaAllDays[col] == np.nan).sum() > 0.5 * len(dfAnovaAllDays):\n",
    "            dfAnovaAllDays.drop(columns=[col], inplace=True)\n",
    "    \n",
    "    # Drop rows where any value is np.nan\n",
    "    dfAnovaAllDays = dfAnovaAllDays[~dfAnovaAllDays.isin([np.nan]).any(axis=1)]\n",
    "    \n",
    "    # Perform friedmann test if there are at least 2 groups (columns)\n",
    "    if len(dfAnovaAllDays.columns) >= 3:\n",
    "        stat, pVal = friedmanchisquare(*[dfAnovaAllDays[col] for col in dfAnovaAllDays.columns])\n",
    "        if pVal < 0.05:  # If Friedman test is significant\n",
    "            posthoc_results, fig  = friedman_posthoc_with_viz(dfAnovaAllDays, dim)\n",
    "            plt.savefig(os.path.join(mainfolder, f\"timestitch_{dim}_all_day_subjective_dim_corr_post_hoc.png\"), bbox_inches='tight', dpi=300)\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "            npAnovaResultsAllDays[dim]['posthoc'] = posthoc_results\n",
    "        else:\n",
    "            npAnovaResultsAllDays[dim]['posthoc'] = []\n",
    "        # Store results\n",
    "        npAnovaResultsAllDays[dim]['stat'] = stat\n",
    "        npAnovaResultsAllDays[dim]['p_val'] = pVal\n",
    "        npAnovaResultsAllDays[dim]['groups'] = list(dfAnovaAllDays.columns)\n",
    "        npAnovaResultsAllDays[dim]['final data length'] = len(dfAnovaAllDays)\n",
    "    else:\n",
    "        # Insufficient data for test\n",
    "        npAnovaResultsAllDays[dim]['stat'] = None\n",
    "        npAnovaResultsAllDays[dim]['p_val'] = None\n",
    "        npAnovaResultsAllDays[dim]['groups'] = list(dfAnovaAllDays.columns)\n",
    "        npAnovaResultsAllDays[dim]['final data length'] = len(dfAnovaAllDays)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e916147-25ed-4ad2-aff5-3bc88557152a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_AnovaResultsAllDays = pd.DataFrame(npAnovaResultsAllDays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff04d94c-e282-41a3-8df2-8374aa20c324",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_AnovaResultsAllDays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b26b31-3d35-4265-ac20-93f645c80dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_AnovaResultsAllDays.to_excel(os.path.join(mainfolder, 'all_day_time_distributions_objective.xlsx'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
